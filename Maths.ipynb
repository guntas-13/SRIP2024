{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$ \\text{Kullback-Leibler Divergence} $$\n",
    "\n",
    "### KL Divergence of Two Distributions of Continuous Random Variable\n",
    "\n",
    "$$ D_{KL}(p \\parallel q) = \\underset{x \\sim p(x)}{\\int} p(x) \\log \\frac{p(x)}{q(x)} $$\n",
    "\n",
    "### KL Divergence of Two Gaussians\n",
    "\n",
    "For a $k$-dimensional Gaussian, the PDF $p(\\mathbf{x})$ is given as:\n",
    "\n",
    "$$ p(\\mathbf{x}) = \\frac{1}{(2 \\pi)^{k/2} | \\Sigma |^{1/2}} \\exp \\left(-\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right) $$\n",
    "\n",
    "Let $p$ and $q$ be two Normal Distributions denoted as $\\mathcal{N}(\\boldsymbol{\\mu}_p, \\Sigma_p)$ and $\\mathcal{N}(\\boldsymbol{\\mu}_q, \\Sigma_q)$ respectively.\n",
    "\n",
    "Then the KL Divergence between these two:\n",
    "\n",
    "$$ D_{KL}(p \\parallel q) = \\mathbb{E}\\_p[ \\log(p) - \\log(q)] $$\n",
    "\n",
    "$$ = \\mathbb{E}_p \\left[ \\frac{1}{2} \\log \\frac{| \\Sigma_q |}{| \\Sigma_p |} - \\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu_p})^T \\Sigma_p^{-1} (\\mathbf{x} - \\boldsymbol{\\mu_p}) + \\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu_q})^T \\Sigma_q^{-1} (\\mathbf{x} - \\boldsymbol{\\mu_q}) \\right] $$\n",
    "\n",
    "$$ = \\frac{1}{2} \\log \\frac{| \\Sigma_q |}{| \\Sigma_p |} - \\mathbb{E}\\_p \\left[\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu_p})^T \\Sigma_p^{-1} (\\mathbf{x} - \\boldsymbol{\\mu_p}) \\right] + \\mathbb{E}\\_p \\left[\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu_q})^T \\Sigma_q^{-1} (\\mathbf{x} - \\boldsymbol{\\mu_q}) \\right] $$\n",
    "\n",
    "Since $(\\mathbf{x} - \\boldsymbol{\\mu_p})^T \\Sigma_p^{-1} (\\mathbf{x} - \\boldsymbol{\\mu_p}) \\in \\mathbb{R}$ so, we write it as $tr \\left\\{(\\mathbf{x} - \\boldsymbol{\\mu_p})^T \\Sigma_p^{-1} (\\mathbf{x} - \\boldsymbol{\\mu_p}) \\right\\}$ and since $tr(\\mathbf{ABC}) = tr(\\mathbf{CBA})$ hence we can also write it as $tr \\left\\{(\\mathbf{x} - \\boldsymbol{\\mu_p}) (\\mathbf{x} - \\boldsymbol{\\mu_p})^T \\Sigma_p^{-1} \\right\\}$. So the second term now is:\n",
    "\n",
    "$$ = \\frac{1}{2} tr \\left\\{ \\mathbb{E}\\_p \\left[(\\mathbf{x} - \\boldsymbol{\\mu_p}) (\\mathbf{x} - \\boldsymbol{\\mu_p})^T \\Sigma_p^{-1} \\right] \\right\\} $$\n",
    "\n",
    "And since $\\mathbb{E}_p \\left[(\\mathbf{x} - \\boldsymbol{\\mu_p}) (\\mathbf{x} - \\boldsymbol{\\mu_p})^T\\right] = \\Sigma_p$. Simplifying we get\n",
    "\n",
    "$$ = \\frac{1}{2} tr \\left\\{ \\Sigma_p \\Sigma_p^{-1} \\right\\} $$\n",
    "$$ = \\frac{1}{2} tr \\left\\{ I_k \\right\\} $$\n",
    "$$ = \\frac{k}{2} $$\n",
    "\n",
    "And simplifying the last term:\n",
    "\n",
    "$$ \\mathbb{E}_p \\left[(\\mathbf{x} - \\boldsymbol{\\mu_q})^T \\Sigma_q^{-1} (\\mathbf{x} - \\boldsymbol{\\mu_q}) \\right] = (\\boldsymbol{\\mu_p} - \\boldsymbol{\\mu_q})^T \\Sigma_q^{-1} (\\boldsymbol{\\mu_p} - \\boldsymbol{\\mu_q}) + tr \\left\\{ \\Sigma_q^{-1} \\Sigma_p \\right\\} $$\n",
    "\n",
    "So, finally\n",
    "\n",
    "### $$ D_{KL}(p \\parallel q) = \\frac{1}{2} \\left[ \\log \\frac{| \\Sigma_q |}{| \\Sigma_p |} - k + (\\boldsymbol{\\mu_p} - \\boldsymbol{\\mu_q})^T \\Sigma_q^{-1} (\\boldsymbol{\\mu_p} - \\boldsymbol{\\mu_q}) + tr \\left\\{ \\Sigma_q^{-1} \\Sigma_p \\right\\} \\right] $$\n",
    "\n",
    "In the scenario when $q$ is $\\mathcal{N}(0, I)$, we get\n",
    "\n",
    "### $$ D_{KL}(p \\parallel q) = \\frac{1}{2} \\left[ \\boldsymbol{\\mu_p}^T \\boldsymbol{\\mu_p} + tr \\left\\{ \\Sigma_p \\right\\} - k - \\log |\\Sigma_p| \\right] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$ \\text{Generative Adversarial Network} $$\n",
    "### $$ \\min_{\\phi} \\max_{\\theta} V(G, D) = \\underset{\\textbf{x} \\sim p_{\\text{data}}}{\\mathbb{E}} [\\log(D_{\\theta}(\\textbf{x}))] + \\underset{\\textbf{z} \\sim p_z(\\textbf{z})}{\\mathbb{E}} [1 - \\log(D_{\\theta}(G_{\\phi}(\\textbf{z})))] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$ \\text{Variational AutoEncoders} $$\n",
    "\n",
    "We wish to achieve two goals:\n",
    "1. **Learning Abstraction** $\\to$ A hidden representation given the input $P(z | X)$ - this is achived by the **Encoder** $Q_{\\theta}(z | X)$.\n",
    "2. **Generation** $\\to$ given some hidden representation using the **Decoder** $P_{\\phi}(X | z)$.\n",
    "\n",
    "For all these our aim to understand the joint distribution $P(X, z) = P(z) \\cdot P(X | z)$. At inference we want given some $X$ (observed variable), finding out the most likely assignments of **latent variables** $z$ which would result in this observation.\n",
    "\n",
    "$$ P(z | X) = \\frac{P(X | z) \\cdot P(z)}{P(X)} $$\n",
    "But since $P(X) = \\int P(X | z) \\cdot P(z) dz = \\int \\int \\dots \\int P(X | z_1, z_2, \\dots z_n) \\cdot P(z_1, z_2, \\dots z_n) dz_1 \\cdot dz_2 \\dots dz_n$ is **intractable**.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VAE.png\" style=\"width:40%; border:0;\">\n",
    "</p>\n",
    "\n",
    "Hence instead, we assume the posterior distribution $P(z | X)$ as $Q_{\\theta}(z | X)$. Further assume that $Q_{\\theta}(z | X)$ is a **Gaussian** whose parameters are determined by our neural network $\\to$ **Encoder**.\n",
    "\n",
    "### $$ \\boldsymbol{\\mu}, \\Sigma = g_{\\theta}(X) $$\n",
    "\n",
    "And since we want this distribution to be close to $P(z | X)$, hence we wish to minimise their KL Divergence.\n",
    "\n",
    "### $$ \\min_{\\theta} D_{KL} \\left(Q_{\\theta}(z | X) \\parallel P(z | X) \\right) $$\n",
    "\n",
    "Since \n",
    "$$ D_{KL} \\left(Q_{\\theta}(z | X) \\parallel P(z | X) \\right) = \\int Q_{\\theta}(z | X) \\log \\frac{Q_{\\theta}(z | X)}{P(z | X)} \\cdot dz $$\n",
    "$$ = \\underset{z \\sim Q_{\\theta}(z | X)}{\\mathbb{E}} \\left[ \\log(Q_{\\theta}(z | X)) - \\log(P(z | X)) \\right] $$\n",
    "$$ = \\mathbb{E}_Q \\left[ \\log(Q_{\\theta}(z | X)) - \\log \\left( \\frac{P(X | z) \\cdot P(z)}{P(X)} \\right) \\right] $$\n",
    "$$ = \\mathbb{E}_Q \\left[ \\log(Q_{\\theta}(z | X)) - \\log(P(z)) \\right] - \\mathbb{E}_Q \\left[ \\log (P(X | z)) \\right] + \\log(P(X)) $$\n",
    "\n",
    "Also since $\\mathbb{E}_Q \\left[ \\log(Q_{\\theta}(z | X)) - \\log(P(z)) \\right] = D_{KL} \\left(Q_{\\theta}(z | X) \\parallel P(z) \\right)$ so, the finally rearranging we may write\n",
    "\n",
    "#### $$ \\log(P(X)) = \\color{red}{D_{KL} \\left(Q_{\\theta}(z | X) \\parallel P(z | X) \\right)} + \\color{blue}{\\mathbb{E}_Q \\left[ \\log (P(X | z)) \\right] - D_{KL} \\left(Q_{\\theta}(z | X) \\parallel P(z) \\right)} $$\n",
    "\n",
    "since $\\color{red}{D_{KL} \\left(Q_{\\theta}(z | X) \\parallel P(z | X) \\right) \\ge 0}$ and $\\color{blue}{\\mathbb{E}_Q \\left[ \\log (P(X | z)) \\right] - D_{KL} \\left(Q_{\\theta}(z | X) \\parallel P(z) \\right)}$ $\\le \\log(P(X))$. And since the final task is maximising the log-likelihood of $P(X)$, hence it is equivalent to maximizing the $\\color{blue}{\\text{Blue Term}}$. So, the final objective is\n",
    "\n",
    "### $$ \\color{green}{\\mathcal{L}(\\theta, \\phi) = \\max_{\\theta, \\phi} \\left\\{ \\mathbb{E}_Q \\left[ \\log (P_{\\phi}(X | z)) \\right] - D_{KL} \\left(Q_{\\theta}(z | X) \\parallel P(z) \\right) \\right \\}} $$\n",
    "\n",
    "Now clearly all the terms are within our reach. To get the KL divergence, we make a forward pass through the **Encoder** to get $Q_{\\theta}(z | X)$ and we know $P(z)$\n",
    "$$ Q_{\\theta}(z | X) \\sim \\mathcal{N}(\\boldsymbol{\\mu_z}(X), \\Sigma_z(X)) $$\n",
    "$$ P(z) \\sim \\mathcal{N}(\\mathbf{0}, I) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
