{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Parameters\n",
    "batch_size = 64\n",
    "num_epochs = 40\n",
    "lr = 1e-4\n",
    "num_grid_rows = 8\n",
    "num_samples = 64\n",
    "\n",
    "# Diffusion Parameters\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "T = 1000\n",
    "\n",
    "# Model Parameters\n",
    "nc = 3\n",
    "image_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize([0.5 for _ in range(nc)], [0.5 for _ in range(nc)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "celebADataset = datasets.ImageFolder(root = \"../CelebA\", transform = transform)\n",
    "celebALoader = DataLoader(dataset = celebADataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "mnist_train = datasets.MNIST(root = \"../MNIST\", train = True, transform = transform, download = True)\n",
    "mnist_test = datasets.MNIST(root = \"../MNIST\", train = False, transform = transform, download = True)\n",
    "\n",
    "mnist_combined_loader = DataLoader(dataset = mnist_train + mnist_test, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Latent Diffusion Models**\n",
    "\n",
    "### The idea is to train the diffusion models on a low dimensional latent representation rather than the entire big pixel space. In addition to that, also train an Encoder-Decoder model that takes the original image converts it into the latent representation using the encoder and reconverts the latent representation to the reconstructed image.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Latent2.png\" style=\"width:60%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "### The downside is that although the $L1/L2$ reconstruction loss might be low, the perceptual features in the reconstructed image still might be **fuzzy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizing the Latent Space using the $\\text{CodeBooks}$ from $\\text{VQVAEs}$\n",
    "### $\\text{VQVAE}$ as the $\\text{AutoEncoder}$\n",
    "\n",
    "$k$ vectors, each of $d$ dimensions $(k \\times d)$ help us encode the data.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE1.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "The encoder generates a feature map of $H \\times W$ features each of $d$ dimension.\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE2.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "For each of the features, we find the nearest $d$ dimensional encoding to it and replace it with that.\n",
    "\n",
    "$$ z_q(x) = e_k $$\n",
    "$$ k = \\argmin_j || z_e(x) - e_j ||_2 $$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE3.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "The decoder then discards off the feature map given by the encoder and only uses the nearest codeblock feature map to reconstruct the output image.\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE4.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "The issue is we have to define the gradients for the $\\argmin$ step separately for the gradients to flow back. We approximate the gradient similar to the straight-through estimator and just copy gradients from decoder input $z_q(x)$ to encoder output $z_e(x)$\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE5.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "$$ L = \\log p(x | z_q(x)) + || \\text{sg}[z_e(x)] - e ||_2^2 + \\beta || z_e(x) - \\text{sg}[e] ||_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptual Retention & $\\text{LPIPS}$ as the metric\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Percept.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"./Media/Latent3.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Latent4.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
