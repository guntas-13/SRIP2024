{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Parameters\n",
    "batch_size = 64\n",
    "num_epochs = 40\n",
    "lr = 1e-4\n",
    "num_grid_rows = 8\n",
    "num_samples = 64\n",
    "\n",
    "# Diffusion Parameters\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "T = 1000\n",
    "\n",
    "# Model Parameters\n",
    "nc = 3\n",
    "image_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize([0.5 for _ in range(nc)], [0.5 for _ in range(nc)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "celebADataset = datasets.ImageFolder(root = \"../CelebA\", transform = transform)\n",
    "celebALoader = DataLoader(dataset = celebADataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "mnist_train = datasets.MNIST(root = \"../MNIST\", train = True, transform = transform, download = True)\n",
    "mnist_test = datasets.MNIST(root = \"../MNIST\", train = False, transform = transform, download = True)\n",
    "\n",
    "mnist_combined_loader = DataLoader(dataset = mnist_train + mnist_test, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Latent Diffusion Models**\n",
    "\n",
    "### The idea is to train the diffusion models on a low dimensional latent representation rather than the entire big pixel space. In addition to that, also train an Encoder-Decoder model that takes the original image converts it into the latent representation using the encoder and reconverts the latent representation to the reconstructed image.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Latent2.png\" style=\"width:60%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "### The downside is that although the $L1/L2$ reconstruction loss might be low, the perceptual features in the reconstructed image still might be **fuzzy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizing the Latent Space using the $\\text{CodeBooks}$ from $\\text{VQVAEs}$\n",
    "### $\\text{VQVAE}$ as the $\\text{AutoEncoder}$\n",
    "\n",
    "$k$ vectors, each of $d$ dimensions $(k \\times d)$ help us encode the data.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE1.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "The encoder generates a feature map of $H \\times W$ features each of $d$ dimension.\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE2.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "For each of the features, we find the nearest $d$ dimensional encoding to it and replace it with that.\n",
    "\n",
    "$$ z_q(x) = e_k $$\n",
    "$$ k = \\argmin_j || z_e(x) - e_j ||_2 $$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE3.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "The decoder then discards off the feature map given by the encoder and only uses the nearest codeblock feature map to reconstruct the output image.\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE4.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "The issue is we have to define the gradients for the $\\argmin$ step separately for the gradients to flow back. We approximate the gradient similar to the straight-through estimator and just copy gradients from decoder input $z_q(x)$ to encoder output $z_e(x)$\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE5.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "$$ L = \\log p(x | z_q(x)) + || \\text{sg}[z_e(x)] - e ||_2^2 + \\beta || z_e(x) - \\text{sg}[e] ||_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptual Retention & $\\text{LPIPS}$ as the metric\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Percept.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "CLearly as said that although the $L_1$ or $L_2$ reconstruction loss might be low for the image, yet the perceptual features in the image perceived by a human are still **blurry**. Now in order to understand how a model would perceive the image, there is no better place to dig into pretrained classification **CNNs** $\\to$ **VGGs**. The goal is to bring the feature map extracted at each VGG layer to be very similar to the original image's feature maps at each VGG layer. This distance metric between the feature maps extracted from the layers of a pretrained VGG is called the **perceptual loss**.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/LPIPS1.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "Check out the original implementation too at [Perceptual Similarity](https://github.com/richzhang/PerceptualSimilarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lpips.py implementation\n",
    "from collections import namedtuple\n",
    "\n",
    "def spatial_average(in_tens, keepdim = True):\n",
    "    return in_tens.mean([2, 3], keepdim = keepdim)\n",
    "\n",
    "\n",
    "class vgg16(nn.Module):\n",
    "    def __init__(self, requires_grad = False, pretrained = True):\n",
    "        super().__init__()\n",
    "        vgg_pretrained_features = models.vgg16(pretrained = pretrained).features\n",
    "        self.slice1 = nn.Sequential()\n",
    "        self.slice2 = nn.Sequential()\n",
    "        self.slice3 = nn.Sequential()\n",
    "        self.slice4 = nn.Sequential()\n",
    "        self.slice5 = nn.Sequential()\n",
    "        self.N_slices = 5\n",
    "        for x in range(4):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "            \n",
    "        for x in range(4, 9):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        for x in range(9, 16):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        for x in range(16, 23):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        for x in range(23, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        # Freeze the model\n",
    "        if requires_grad == False:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu1_2 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2_2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3_3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4_3 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5_3 = h\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\", \"relu5_3\"])\n",
    "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ScalingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # mean = [0.485, 0.456, 0.406]\n",
    "        # std = [0.229, 0.224, 0.225]\n",
    "        self.register_buffer(\"shift\", torch.tensor([-.030, -.088, -.188])[None, :, None, None])\n",
    "        self.register_buffer(\"scale\", torch.tensor([.458, .448, .450])[None, :, None, None])\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return (inp - self.shift) / self.scale\n",
    "\n",
    "class NetLinLayer(nn.Module):\n",
    "    def __init__(self, chn_in, chn_out = 1, use_dropout = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.Dropout(), ] if (use_dropout) else []\n",
    "        layers += [nn.Conv2d(chn_in, chn_out, kernel_size = 1, stride = 1, padding = 0, bias = False), ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class LPIPS(nn.Module):\n",
    "    def __init__(self, net = \"vgg\", version = \"0.1\", use_dropout = True):\n",
    "        super().__init__()\n",
    "        self.scaling_layer = ScalingLayer()\n",
    "        \n",
    "        self.chns = [64, 128, 256, 512, 512]\n",
    "        self.L = len(self.chns)\n",
    "        self.net = vgg16(pretrained = True, requires_grad = False)\n",
    "        \n",
    "        self.lin0 = NetLinLayer(self.chns[0], use_dropout = use_dropout)\n",
    "        self.lin1 = NetLinLayer(self.chns[1], use_dropout = use_dropout)\n",
    "        self.lin2 = NetLinLayer(self.chns[2], use_dropout = use_dropout)\n",
    "        self.lin3 = NetLinLayer(self.chns[3], use_dropout = use_dropout)\n",
    "        self.lin4 = NetLinLayer(self.chns[4], use_dropout = use_dropout)\n",
    "        self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n",
    "        self.lins = nn.ModuleList(self.lins)\n",
    "        \n",
    "        import inspect\n",
    "        \n",
    "        model_path = os.path.abspath(\n",
    "            os.path.join(inspect.getfile(self.__init__), \"..\", \"weights/v%s/%s.pth\" % (version, net))\n",
    "        )\n",
    "        print(\"Loading model from: %s\" % model_path)\n",
    "        self.net.load_state_dict(torch.load(model_path, map_location = device), strict = False)\n",
    "        \n",
    "        self.eval()\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    \n",
    "    def forward(self, in0, in1, normalize = False):\n",
    "        if normalize:\n",
    "            in0 = 2 * in0 - 1\n",
    "            in1 = 2 * in1 - 1\n",
    "        \n",
    "        in0_input, in1_input = self.scaling_layer(in0), self.scaling_layer(in1)\n",
    "        \n",
    "        outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)\n",
    "        \n",
    "        feats0, feats1, diffs = {}, {}, {}\n",
    "        \n",
    "        for kk in range(self.L):\n",
    "            feats0[kk], feats1[kk] = F.normalize(outs0[kk], dim = 1), F.normalize(outs1[kk])\n",
    "            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n",
    "        \n",
    "        res = [spatial_average(self.lins[kk](diffs[kk]), keepdim = True) for kk in range(self.L)]\n",
    "        val = 0\n",
    "        \n",
    "        for l in range(self.L):\n",
    "            val += res[l]\n",
    "        \n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"./Media/Latent3.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Latent4.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
