{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import glob\n",
    "import pickle\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Latent Diffusion Models**\n",
    "\n",
    "### The idea is to train the diffusion models on a low dimensional latent representation rather than the entire big pixel space. In addition to that, also train an Encoder-Decoder model that takes the original image converts it into the latent representation using the encoder and reconverts the latent representation to the reconstructed image.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Latent2.png\" style=\"width:60%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "### The downside is that although the $L1/L2$ reconstruction loss might be low, the perceptual features in the reconstructed image still might be **fuzzy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptual Retention & $\\text{LPIPS}$ as the metric\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Percept.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "CLearly as said that although the $L_1$ or $L_2$ reconstruction loss might be low for the image, yet the perceptual features in the image perceived by a human are still **blurry**. Now in order to understand how a model would perceive the image, there is no better place to dig into pretrained classification **CNNs** $\\to$ **VGGs**. The goal is to bring the feature map extracted at each VGG layer to be very similar to the original image's feature maps at each VGG layer. This distance metric between the feature maps extracted from the layers of a pretrained VGG is called the **perceptual loss**.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/LPIPS1.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "Check out the original implementation too at [Perceptual Similarity](https://github.com/richzhang/PerceptualSimilarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lpips.py implementation\n",
    "from collections import namedtuple\n",
    "\n",
    "def spatial_average(in_tens, keepdim = True):\n",
    "    return in_tens.mean([2, 3], keepdim = keepdim)\n",
    "\n",
    "\n",
    "class vgg16(nn.Module):\n",
    "    def __init__(self, requires_grad = False, pretrained = True):\n",
    "        super().__init__()\n",
    "        vgg_pretrained_features = models.vgg16(pretrained = pretrained).features\n",
    "        self.slice1 = nn.Sequential()\n",
    "        self.slice2 = nn.Sequential()\n",
    "        self.slice3 = nn.Sequential()\n",
    "        self.slice4 = nn.Sequential()\n",
    "        self.slice5 = nn.Sequential()\n",
    "        self.N_slices = 5\n",
    "        for x in range(4):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "            \n",
    "        for x in range(4, 9):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        for x in range(9, 16):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        for x in range(16, 23):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        for x in range(23, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        # Freeze the model\n",
    "        if requires_grad == False:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu1_2 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2_2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3_3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4_3 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5_3 = h\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\", \"relu5_3\"])\n",
    "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ScalingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # mean = [0.485, 0.456, 0.406]\n",
    "        # std = [0.229, 0.224, 0.225]\n",
    "        self.register_buffer(\"shift\", torch.tensor([-.030, -.088, -.188])[None, :, None, None])\n",
    "        self.register_buffer(\"scale\", torch.tensor([.458, .448, .450])[None, :, None, None])\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return (inp - self.shift) / self.scale\n",
    "\n",
    "class NetLinLayer(nn.Module):\n",
    "    def __init__(self, chn_in, chn_out = 1, use_dropout = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.Dropout(), ] if (use_dropout) else []\n",
    "        layers += [nn.Conv2d(chn_in, chn_out, kernel_size = 1, stride = 1, padding = 0, bias = False), ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class LPIPS(nn.Module):\n",
    "    def __init__(self, use_dropout = True):\n",
    "        super().__init__()\n",
    "        self.scaling_layer = ScalingLayer()\n",
    "        \n",
    "        self.chns = [64, 128, 256, 512, 512]\n",
    "        self.L = len(self.chns)\n",
    "        self.net = vgg16(pretrained = True, requires_grad = False)\n",
    "        \n",
    "        self.lin0 = NetLinLayer(self.chns[0], use_dropout = use_dropout)\n",
    "        self.lin1 = NetLinLayer(self.chns[1], use_dropout = use_dropout)\n",
    "        self.lin2 = NetLinLayer(self.chns[2], use_dropout = use_dropout)\n",
    "        self.lin3 = NetLinLayer(self.chns[3], use_dropout = use_dropout)\n",
    "        self.lin4 = NetLinLayer(self.chns[4], use_dropout = use_dropout)\n",
    "        self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n",
    "        self.lins = nn.ModuleList(self.lins)\n",
    "        \n",
    "        self.net.load_state_dict(torch.load(\"./vgg.pth\", map_location = device), strict = False)\n",
    "        \n",
    "        self.eval()\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    \n",
    "    def forward(self, in0, in1, normalize = False):\n",
    "        if normalize:\n",
    "            in0 = 2 * in0 - 1\n",
    "            in1 = 2 * in1 - 1\n",
    "        \n",
    "        in0_input, in1_input = self.scaling_layer(in0), self.scaling_layer(in1)\n",
    "        \n",
    "        outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)\n",
    "        \n",
    "        feats0, feats1, diffs = {}, {}, {}\n",
    "        \n",
    "        for kk in range(self.L):\n",
    "            feats0[kk], feats1[kk] = F.normalize(outs0[kk], dim = 1), F.normalize(outs1[kk])\n",
    "            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n",
    "        \n",
    "        res = [spatial_average(self.lins[kk](diffs[kk]), keepdim = True) for kk in range(self.L)]\n",
    "        val = 0\n",
    "        \n",
    "        for l in range(self.L):\n",
    "            val += res[l]\n",
    "        \n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizing the Latent Space using the $\\text{CodeBooks}$ from $\\text{VQVAEs}$\n",
    "### $\\text{VQVAE}$ as the $\\text{AutoEncoder}$\n",
    "\n",
    "$k$ vectors, each of $d$ dimensions $(k \\times d)$ help us encode the data.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE1.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "The encoder generates a feature map of $H \\times W$ features each of $d$ dimension.\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE2.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "For each of the features, we find the nearest $d$ dimensional encoding to it and replace it with that.\n",
    "\n",
    "$$ z_q(x) = e_k $$\n",
    "$$ k = \\argmin_j || z_e(x) - e_j ||_2 $$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE3.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "The decoder then discards off the feature map given by the encoder and only uses the nearest codeblock feature map to reconstruct the output image.\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE4.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "The issue is we have to define the gradients for the $\\argmin$ step separately for the gradients to flow back. We approximate the gradient similar to the straight-through estimator and just copy gradients from decoder input $z_q(x)$ to encoder output $z_e(x)$\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE5.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "$$ L = \\log p(x | z_q(x)) + || \\text{sg}[z_e(x)] - e ||_2^2 + \\beta || z_e(x) - \\text{sg}[e] ||_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The AutoEncoder Architecture\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Latent3.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Latent4.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/AutoEnc1.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/AutoEnc2.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/AutoEnc3.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/AutoEnc4.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model Blocks\n",
    "Adapted from [ExplainingAI](https://github.com/explainingai-code/StableDiffusion-PyTorch/blob/main/models/blocks.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Embedding\n",
    "\n",
    "def get_time_embedding(T, d_model):\n",
    "    factor = 10000 ** ((torch.arange(start = 0, end = d_model // 2, dtype = torch.float32, device = T.device)) / (d_model // 2))\n",
    "    t_emb = T[:, None].repeat(1, d_model // 2) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim = -1)\n",
    "    return t_emb\n",
    "\n",
    "\n",
    "# Model Blocks\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Down conv block with attention.\n",
    "    Sequence of following block\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Downsample\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 down_sample, num_heads, num_layers, attn, norm_channels, cross_attn=False, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.down_sample = down_sample\n",
    "        self.attn = attn\n",
    "        self.context_dim = context_dim\n",
    "        self.cross_attn = cross_attn\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(self.t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.attn:\n",
    "            self.attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            \n",
    "            self.attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "        \n",
    "        if self.cross_attn:\n",
    "            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n",
    "            self.cross_attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.cross_attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.context_proj = nn.ModuleList(\n",
    "                [nn.Linear(context_dim, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n",
    "                                          4, 2, 1) if self.down_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t_emb=None, context=None):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet block of Unet\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            if self.attn:\n",
    "                # Attention block of Unet\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "            \n",
    "            if self.cross_attn:\n",
    "                assert context is not None, \"context cannot be None if cross attention layers are used\"\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.cross_attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n",
    "                context_proj = self.context_proj[i](context)\n",
    "                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "            \n",
    "        # Downsample\n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MidBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Mid conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Resnet block with time embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads, num_layers, norm_channels, cross_attn=None, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.context_dim = context_dim\n",
    "        self.cross_attn = cross_attn\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers + 1)\n",
    "            ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(norm_channels, out_channels)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.attentions = nn.ModuleList(\n",
    "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        if self.cross_attn:\n",
    "            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n",
    "            self.cross_attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.cross_attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.context_proj = nn.ModuleList(\n",
    "                [nn.Linear(context_dim, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t_emb=None, context=None):\n",
    "        out = x\n",
    "        \n",
    "        # First resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        if self.t_emb_dim is not None:\n",
    "            out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.residual_input_conv[0](resnet_input)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            # Attention Block\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "            \n",
    "            if self.cross_attn:\n",
    "                assert context is not None, \"context cannot be None if cross attention layers are used\"\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.cross_attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n",
    "                context_proj = self.context_proj[i](context)\n",
    "                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "                \n",
    "            \n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i + 1](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i + 1](out)\n",
    "            out = out + self.residual_input_conv[i + 1](resnet_input)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Up conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Upsample\n",
    "    1. Concatenate Down block output\n",
    "    2. Resnet block with time embedding\n",
    "    3. Attention Block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 up_sample, num_heads, num_layers, attn, norm_channels):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.attn = attn\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "        \n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        if self.attn:\n",
    "            self.attention_norms = nn.ModuleList(\n",
    "                [\n",
    "                    nn.GroupNorm(norm_channels, out_channels)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            self.attentions = nn.ModuleList(\n",
    "                [\n",
    "                    nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(in_channels, in_channels,\n",
    "                                                 4, 2, 1) \\\n",
    "            if self.up_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, out_down=None, t_emb=None):\n",
    "        # Upsample\n",
    "        x = self.up_sample_conv(x)\n",
    "        \n",
    "        # Concat with Downblock output\n",
    "        if out_down is not None:\n",
    "            x = torch.cat([x, out_down], dim=1)\n",
    "        \n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            # Self Attention\n",
    "            if self.attn:\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpBlockUnet(nn.Module):\n",
    "    r\"\"\"\n",
    "    Up conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Upsample\n",
    "    1. Concatenate Down block output\n",
    "    2. Resnet block with time embedding\n",
    "    3. Attention Block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample,\n",
    "                 num_heads, num_layers, norm_channels, cross_attn=False, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.cross_attn = cross_attn\n",
    "        self.context_dim = context_dim\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "            \n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [\n",
    "                nn.GroupNorm(norm_channels, out_channels)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attentions = nn.ModuleList(\n",
    "            [\n",
    "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.cross_attn:\n",
    "            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n",
    "            self.cross_attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.cross_attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.context_proj = nn.ModuleList(\n",
    "                [nn.Linear(context_dim, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n",
    "                                                 4, 2, 1) \\\n",
    "            if self.up_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, out_down=None, t_emb=None, context=None):\n",
    "        x = self.up_sample_conv(x)\n",
    "        if out_down is not None:\n",
    "            x = torch.cat([x, out_down], dim=1)\n",
    "        \n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            # Self Attention\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "            # Cross Attention\n",
    "            if self.cross_attn:\n",
    "                assert context is not None, \"context cannot be None if cross attention layers are used\"\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.cross_attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                assert len(context.shape) == 3, \\\n",
    "                    \"Context shape does not match B,_,CONTEXT_DIM\"\n",
    "                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim,\\\n",
    "                    \"Context shape does not match B,_,CONTEXT_DIM\"\n",
    "                context_proj = self.context_proj[i](context)\n",
    "                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQVAE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        im_channels = 3\n",
    "        self.down_channels = [64, 128, 256, 256]\n",
    "        self.mid_channels = [256, 256]\n",
    "        self.down_sample = [True, True, True]\n",
    "        self.num_down_layers = 2\n",
    "        self.num_mid_layers = 2\n",
    "        self.num_up_layers = 2\n",
    "        self.norm_channels = 32\n",
    "        \n",
    "        # To disable attention in the DownBlock of Encoder and UpBlock of Decoder\n",
    "        self.attns = [False, False, False]\n",
    "        self.num_heads = 4\n",
    "        \n",
    "        # Latent Dimension\n",
    "        self.z_channels = 3\n",
    "        self.codebook_size = 8192\n",
    "        \n",
    "        # Wherever we use downsampling in encoder correspondingly use\n",
    "        # upsampling in decoder\n",
    "        self.up_sample = list(reversed(self.down_sample))\n",
    "        \n",
    "        ##################### Encoder ######################\n",
    "        self.encoder_conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1))\n",
    "        \n",
    "        # Downblock + Midblock\n",
    "        self.encoder_layers = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels) - 1):\n",
    "            self.encoder_layers.append(DownBlock(self.down_channels[i], self.down_channels[i + 1],\n",
    "                                                 t_emb_dim=None, down_sample=self.down_sample[i],\n",
    "                                                 num_heads=self.num_heads,\n",
    "                                                 num_layers=self.num_down_layers,\n",
    "                                                 attn=self.attns[i],\n",
    "                                                 norm_channels=self.norm_channels))\n",
    "        \n",
    "        self.encoder_mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels) - 1):\n",
    "            self.encoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1],\n",
    "                                              t_emb_dim=None,\n",
    "                                              num_heads=self.num_heads,\n",
    "                                              num_layers=self.num_mid_layers,\n",
    "                                              norm_channels=self.norm_channels))\n",
    "        \n",
    "        self.encoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[-1])\n",
    "        self.encoder_conv_out = nn.Conv2d(self.down_channels[-1], self.z_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Pre Quantization Convolution\n",
    "        self.pre_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size=1)\n",
    "        \n",
    "        # Codebook\n",
    "        self.embedding = nn.Embedding(self.codebook_size, self.z_channels)\n",
    "        ####################################################\n",
    "        \n",
    "        ##################### Decoder ######################\n",
    "        \n",
    "        # Post Quantization Convolution\n",
    "        self.post_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size=1)\n",
    "        self.decoder_conv_in = nn.Conv2d(self.z_channels, self.mid_channels[-1], kernel_size=3, padding=(1, 1))\n",
    "        \n",
    "        # Midblock + Upblock\n",
    "        self.decoder_mids = nn.ModuleList([])\n",
    "        for i in reversed(range(1, len(self.mid_channels))):\n",
    "            self.decoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i - 1],\n",
    "                                              t_emb_dim=None,\n",
    "                                              num_heads=self.num_heads,\n",
    "                                              num_layers=self.num_mid_layers,\n",
    "                                              norm_channels=self.norm_channels))\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([])\n",
    "        for i in reversed(range(1, len(self.down_channels))):\n",
    "            self.decoder_layers.append(UpBlock(self.down_channels[i], self.down_channels[i - 1],\n",
    "                                               t_emb_dim=None, up_sample=self.down_sample[i - 1],\n",
    "                                               num_heads=self.num_heads,\n",
    "                                               num_layers=self.num_up_layers,\n",
    "                                               attn=self.attns[i-1],\n",
    "                                               norm_channels=self.norm_channels))\n",
    "        \n",
    "        self.decoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[0])\n",
    "        self.decoder_conv_out = nn.Conv2d(self.down_channels[0], im_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def quantize(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # B, C, H, W -> B, H, W, C\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        \n",
    "        # B, H, W, C -> B, H*W, C\n",
    "        x = x.reshape(x.size(0), -1, x.size(-1))\n",
    "        \n",
    "        # Find nearest embedding/codebook vector\n",
    "        # dist between (B, H*W, C) and (B, K, C) -> (B, H*W, K)\n",
    "        dist = torch.cdist(x, self.embedding.weight[None, :].repeat((x.size(0), 1, 1)))\n",
    "        # (B, H*W)\n",
    "        min_encoding_indices = torch.argmin(dist, dim=-1)\n",
    "        \n",
    "        # Replace encoder output with nearest codebook\n",
    "        # quant_out -> B*H*W, C\n",
    "        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1))\n",
    "        \n",
    "        # x -> B*H*W, C\n",
    "        x = x.reshape((-1, x.size(-1)))\n",
    "        commmitment_loss = torch.mean((quant_out.detach() - x) ** 2)\n",
    "        codebook_loss = torch.mean((quant_out - x.detach()) ** 2)\n",
    "        quantize_losses = {\n",
    "            'codebook_loss': codebook_loss,\n",
    "            'commitment_loss': commmitment_loss\n",
    "        }\n",
    "        # Straight through estimation\n",
    "        quant_out = x + (quant_out - x).detach()\n",
    "        \n",
    "        # quant_out -> B, C, H, W\n",
    "        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2)\n",
    "        min_encoding_indices = min_encoding_indices.reshape((-1, quant_out.size(-2), quant_out.size(-1)))\n",
    "        return quant_out, quantize_losses, min_encoding_indices\n",
    "\n",
    "    def encode(self, x):\n",
    "        out = self.encoder_conv_in(x)\n",
    "        for idx, down in enumerate(self.encoder_layers):\n",
    "            out = down(out)\n",
    "        for mid in self.encoder_mids:\n",
    "            out = mid(out)\n",
    "        out = self.encoder_norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.encoder_conv_out(out)\n",
    "        out = self.pre_quant_conv(out)\n",
    "        out, quant_losses, _ = self.quantize(out)\n",
    "        return out, quant_losses\n",
    "    \n",
    "    def decode(self, z):\n",
    "        out = z\n",
    "        out = self.post_quant_conv(out)\n",
    "        out = self.decoder_conv_in(out)\n",
    "        for mid in self.decoder_mids:\n",
    "            out = mid(out)\n",
    "        for idx, up in enumerate(self.decoder_layers):\n",
    "            out = up(out)\n",
    "        \n",
    "        out = self.decoder_norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.decoder_conv_out(out)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z, quant_losses = self.encode(x)\n",
    "        out = self.decode(z)\n",
    "        return out, z, quant_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    r\"\"\"\n",
    "    PatchGAN Discriminator.\n",
    "    Rather than taking IMG_CHANNELSxIMG_HxIMG_W all the way to\n",
    "    1 scalar value , we instead predict grid of values.\n",
    "    Where each grid is prediction of how likely\n",
    "    the discriminator thinks that the image patch corresponding\n",
    "    to the grid cell is real\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, im_channels=3,\n",
    "                 conv_channels=[64, 128, 256],\n",
    "                 kernels=[4,4,4,4],\n",
    "                 strides=[2,2,2,1],\n",
    "                 paddings=[1,1,1,1]):\n",
    "        super().__init__()\n",
    "        self.im_channels = im_channels\n",
    "        activation = nn.LeakyReLU(0.2)\n",
    "        layers_dim = [self.im_channels] + conv_channels + [1]\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(layers_dim[i], layers_dim[i + 1],\n",
    "                          kernel_size=kernels[i],\n",
    "                          stride=strides[i],\n",
    "                          padding=paddings[i],\n",
    "                          bias=False if i !=0 else True),\n",
    "                nn.BatchNorm2d(layers_dim[i + 1]) if i != len(layers_dim) - 2 and i != 0 else nn.Identity(),\n",
    "                activation if i != len(layers_dim) - 2 else nn.Identity()\n",
    "            )\n",
    "            for i in range(len(layers_dim) - 1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Parameters\n",
    "batch_size_ldm = 16\n",
    "batch_size_autoenc = 4\n",
    "num_epochs_autoenc = 20\n",
    "num_epochs_ldm = 100\n",
    "\n",
    "# Model Parameters\n",
    "nc = 3\n",
    "image_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset - CelebAHQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../CelebAMask-HQ\"\n",
    "\n",
    "def load_latents(latent_path):\n",
    "    r\"\"\"\n",
    "    Simple utility to save latents to speed up ldm training\n",
    "    :param latent_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    latent_maps = {}\n",
    "    for fname in glob.glob(os.path.join(latent_path, '*.pkl')):\n",
    "        s = pickle.load(open(fname, 'rb'))\n",
    "        for k, v in s.items():\n",
    "            latent_maps[k] = v[0]\n",
    "    return latent_maps\n",
    "\n",
    "class CelebDataset(Dataset):\n",
    "    r\"\"\"\n",
    "    Celeb dataset will by default centre crop and resize the images.\n",
    "    This can be replaced by any other dataset. As long as all the images\n",
    "    are under one directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, split, im_path, im_size=256, im_channels=3, im_ext='jpg',\n",
    "                 use_latents=False, latent_path=None, condition_config=None):\n",
    "        self.split = split\n",
    "        self.im_size = im_size\n",
    "        self.im_channels = im_channels\n",
    "        self.im_ext = im_ext\n",
    "        self.im_path = im_path\n",
    "        self.latent_maps = None\n",
    "        self.use_latents = False\n",
    "        \n",
    "        self.condition_types = [] if condition_config is None else condition_config['condition_types']\n",
    "        \n",
    "        self.idx_to_cls_map = {}\n",
    "        self.cls_to_idx_map ={}\n",
    "        \n",
    "        if 'image' in self.condition_types:\n",
    "            self.mask_channels = condition_config['image_condition_config']['image_condition_input_channels']\n",
    "            self.mask_h = condition_config['image_condition_config']['image_condition_h']\n",
    "            self.mask_w = condition_config['image_condition_config']['image_condition_w']\n",
    "            \n",
    "        self.images, self.texts, self.masks = self.load_images(im_path)\n",
    "        \n",
    "        # Whether to load images or to load latents\n",
    "        if use_latents and latent_path is not None:\n",
    "            latent_maps = load_latents(latent_path)\n",
    "            if len(latent_maps) == len(self.images):\n",
    "                self.use_latents = True\n",
    "                self.latent_maps = latent_maps\n",
    "                print('Found {} latents'.format(len(self.latent_maps)))\n",
    "            else:\n",
    "                print('Latents not found')\n",
    "    \n",
    "    def load_images(self, im_path):\n",
    "        r\"\"\"\n",
    "        Gets all images from the path specified\n",
    "        and stacks them all up\n",
    "        \"\"\"\n",
    "        assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n",
    "        ims = []\n",
    "        fnames = glob.glob(os.path.join(im_path, 'CelebA-HQ-img/*.{}'.format('png')))\n",
    "        fnames += glob.glob(os.path.join(im_path, 'CelebA-HQ-img/*.{}'.format('jpg')))\n",
    "        fnames += glob.glob(os.path.join(im_path, 'CelebA-HQ-img/*.{}'.format('jpeg')))\n",
    "        texts = []\n",
    "        masks = []\n",
    "        \n",
    "        if 'image' in self.condition_types:\n",
    "            label_list = ['skin', 'nose', 'eye_g', 'l_eye', 'r_eye', 'l_brow', 'r_brow', 'l_ear', 'r_ear', 'mouth',\n",
    "                          'u_lip', 'l_lip', 'hair', 'hat', 'ear_r', 'neck_l', 'neck', 'cloth']\n",
    "            self.idx_to_cls_map = {idx: label_list[idx] for idx in range(len(label_list))}\n",
    "            self.cls_to_idx_map = {label_list[idx]: idx for idx in range(len(label_list))}\n",
    "        \n",
    "        for fname in tqdm(fnames):\n",
    "            ims.append(fname)\n",
    "            \n",
    "            if 'text' in self.condition_types:\n",
    "                im_name = os.path.split(fname)[1].split('.')[0]\n",
    "                captions_im = []\n",
    "                with open(os.path.join(im_path, 'celeba-caption/{}.txt'.format(im_name))) as f:\n",
    "                    for line in f.readlines():\n",
    "                        captions_im.append(line.strip())\n",
    "                texts.append(captions_im)\n",
    "                \n",
    "            if 'image' in self.condition_types:\n",
    "                im_name = int(os.path.split(fname)[1].split('.')[0])\n",
    "                masks.append(os.path.join(im_path, 'CelebAMask-HQ-mask', '{}.png'.format(im_name)))\n",
    "        if 'text' in self.condition_types:\n",
    "            assert len(texts) == len(ims), \"Condition Type Text but could not find captions for all images\"\n",
    "        if 'image' in self.condition_types:\n",
    "            assert len(masks) == len(ims), \"Condition Type Image but could not find masks for all images\"\n",
    "        print('Found {} images'.format(len(ims)))\n",
    "        print('Found {} masks'.format(len(masks)))\n",
    "        print('Found {} captions'.format(len(texts)))\n",
    "        return ims, texts, masks\n",
    "    \n",
    "    def get_mask(self, index):\n",
    "        r\"\"\"\n",
    "        Method to get the mask of WxH\n",
    "        for given index and convert it into\n",
    "        Classes x W x H mask image\n",
    "        :param index:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        mask_im = Image.open(self.masks[index])\n",
    "        mask_im = np.array(mask_im)\n",
    "        im_base = np.zeros((self.mask_h, self.mask_w, self.mask_channels))\n",
    "        for orig_idx in range(len(self.idx_to_cls_map)):\n",
    "            im_base[mask_im == (orig_idx+1), orig_idx] = 1\n",
    "        mask = torch.from_numpy(im_base).permute(2, 0, 1).float()\n",
    "        return mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ######## Set Conditioning Info ########\n",
    "        cond_inputs = {}\n",
    "        if 'text' in self.condition_types:\n",
    "            cond_inputs['text'] = random.sample(self.texts[index], k=1)[0]\n",
    "        if 'image' in self.condition_types:\n",
    "            mask = self.get_mask(index)\n",
    "            cond_inputs['image'] = mask\n",
    "        #######################################\n",
    "        \n",
    "        if self.use_latents:\n",
    "            latent = self.latent_maps[self.images[index]]\n",
    "            if len(self.condition_types) == 0:\n",
    "                return latent\n",
    "            else:\n",
    "                return latent, cond_inputs\n",
    "        else:\n",
    "            im = Image.open(self.images[index])\n",
    "            im_tensor = transforms.Compose([\n",
    "                transforms.Resize(self.im_size),\n",
    "                transforms.CenterCrop(self.im_size),\n",
    "                transforms.ToTensor(),\n",
    "            ])(im)\n",
    "            im.close()\n",
    "        \n",
    "            # Convert input to -1 to 1 range.\n",
    "            im_tensor = (2 * im_tensor) - 1\n",
    "            if len(self.condition_types) == 0:\n",
    "                return im_tensor\n",
    "            else:\n",
    "                return im_tensor, cond_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30000/30000 [00:00<00:00, 5883989.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30000 images\n",
      "Found 0 masks\n",
      "Found 0 captions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "im_dataset = CelebDataset(split='train',\n",
    "                                im_path=path,\n",
    "                                im_size=image_size,\n",
    "                                im_channels=nc)\n",
    "\n",
    "celebALoader = DataLoader(im_dataset,\n",
    "                             batch_size=batch_size_autoenc,\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQVAE().to(device)\n",
    "lpips_model = LPIPS().eval().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "recon_criterion = nn.MSELoss()\n",
    "disc_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr = 1e-5, betas = (0.5, 0.999))\n",
    "optimizer_g = torch.optim.Adam(model.parameters(), lr = 1e-5, betas = (0.5, 0.999))\n",
    "\n",
    "disc_step_start = 15000\n",
    "step_count = 0\n",
    "acc_steps = 4\n",
    "\n",
    "\n",
    "for epoch_idx in range(num_epochs_autoenc):\n",
    "    real_images = []\n",
    "    reconstructed_images = []\n",
    "    quantized_images = []\n",
    "    recon_losses = []\n",
    "    codebook_losses = []\n",
    "    perceptual_losses = []\n",
    "    disc_losses = []\n",
    "    gen_losses = []\n",
    "    losses = []\n",
    "    \n",
    "    optimizer_d.zero_grad()\n",
    "    optimizer_g.zero_grad()\n",
    "    \n",
    "    for im in tqdm(celebALoader):\n",
    "        \n",
    "        step_count += 1\n",
    "        im = im.float().to(device)\n",
    "        \n",
    "        # Generator\n",
    "        model_output = model(im)\n",
    "        output, z, quantize_losses = model_output\n",
    "        \n",
    "        recon_loss = recon_criterion(output, im)\n",
    "        recon_losses.append(recon_loss.item())\n",
    "        recon_loss = recon_loss / acc_steps\n",
    "        \n",
    "        g_loss = recon_loss + (1 * quantize_losses[\"codebook_loss\"] / acc_steps) + (0.2 * quantize_losses[\"commitment_loss\"] / acc_steps)\n",
    "        \n",
    "        codebook_losses.append(quantize_losses[\"codebook_loss\"].item())\n",
    "        \n",
    "        # Adversarial Loss\n",
    "        if step_count > disc_step_start:\n",
    "            disc_fake_pred = discriminator(output)\n",
    "            disc_fake_loss = disc_criterion(disc_fake_pred, torch.ones(disc_fake_pred.shape, device = disc_fake_pred.device))\n",
    "            gen_losses.append(0.5 * disc_fake_loss.item())\n",
    "            g_loss += 0.5 * disc_fake_loss.item()\n",
    "        \n",
    "        lpips_loss = torch.mean(lpips_model(output, im)) / acc_steps\n",
    "        perceptual_losses.append(lpips_loss.item())\n",
    "        g_loss += lpips_loss / acc_steps\n",
    "        losses.append(g_loss.item())\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        # Discriminator\n",
    "        if step_count > disc_step_start:\n",
    "            fake = output\n",
    "            disc_fake_pred = discriminator(fake.detach())\n",
    "            disc_real_pred = discriminator(im)\n",
    "            disc_fake_loss = disc_criterion(disc_fake_pred, torch.zeros(disc_fake_pred.shape, device = disc_fake_pred.device))\n",
    "            disc_real_loss = disc_criterion(disc_real_pred, torch.ones(disc_real_pred.shape, device = disc_real_pred.device))\n",
    "            \n",
    "            disc_loss = 0.5 * (disc_fake_loss + disc_real_loss) / 2\n",
    "            disc_losses.append(disc_loss.item())\n",
    "            disc_loss = disc_loss / acc_steps\n",
    "            disc_loss.backward()\n",
    "            if step_count % acc_steps == 0:\n",
    "                optimizer_d.step()\n",
    "                optimizer_d.zero_grad()\n",
    "        \n",
    "        if step_count % acc_steps == 0:\n",
    "            optimizer_g.step()\n",
    "            optimizer_g.zero_grad()\n",
    "        \n",
    "        if len(real_images) < 16:\n",
    "            real_images.append(im)\n",
    "            reconstructed_images.append(output)\n",
    "            quantized_images.append(z)\n",
    "    \n",
    "    optimizer_d.step()\n",
    "    optimizer_d.zero_grad()\n",
    "    optimizer_g.step()\n",
    "    optimizer_g.zero_grad()\n",
    "    \n",
    "    \n",
    "    if len(disc_losses) > 0:\n",
    "        print(\n",
    "            'Finished epoch: {} | Recon Loss : {:.4f} | Perceptual Loss : {:.4f} | '\n",
    "            'Codebook : {:.4f} | G Loss : {:.4f} | D Loss {:.4f}'.\n",
    "            format(epoch_idx + 1,\n",
    "                    np.mean(recon_losses),\n",
    "                    np.mean(perceptual_losses),\n",
    "                    np.mean(codebook_losses),\n",
    "                    np.mean(gen_losses),\n",
    "                    np.mean(disc_losses)))\n",
    "    else:\n",
    "        print('Finished epoch: {} | Recon Loss : {:.4f} | Perceptual Loss : {:.4f} | Codebook : {:.4f}'.\n",
    "                format(epoch_idx + 1,\n",
    "                        np.mean(recon_losses),\n",
    "                        np.mean(perceptual_losses),\n",
    "                        np.mean(codebook_losses)))\n",
    "        \n",
    "    torch.save(model.state_dict(), \"../vqvaeCeleb/vqvae_autoencoder.pth\")\n",
    "    torch.save(discriminator.state_dict(), \"../vqvaeCeleb/vqvae_discriminator.pth\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        real_images = torch.cat(real_images[:16], dim=0)\n",
    "        reconstructed_images = torch.cat(reconstructed_images[:16], dim=0)\n",
    "        quantized_images = torch.cat(quantized_images[:16], dim=0)\n",
    "        \n",
    "        grid_real = vutils.make_grid(real_images, nrow = 4, normalize = True)\n",
    "        grid_reconstructed = vutils.make_grid(reconstructed_images, nrow = 4, normalize = True)\n",
    "        grid_quantized = vutils.make_grid(quantized_images, nrow = 4, normalize = True)\n",
    "        \n",
    "        plt.figure(figsize = (15, 15))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Real Images\")\n",
    "        plt.imshow(np.transpose(grid_real.cpu().detach().numpy(), (1, 2, 0)))\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Reconstructed Images\")\n",
    "        plt.imshow(np.transpose(grid_reconstructed.cpu().detach().numpy(), (1, 2, 0)))\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Quantized Images\")\n",
    "        plt.imshow(np.transpose(grid_quantized.cpu().detach().numpy(), (1, 2, 0)))\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer from VQVAE to get latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dataset = CelebDataset(split='train',\n",
    "                                im_path=path,\n",
    "                                im_size=image_size,\n",
    "                                im_channels=nc)\n",
    "data_loader = DataLoader(im_dataset,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False)\n",
    "\n",
    "num_images = 1\n",
    "ngrid = 1\n",
    "\n",
    "idxs = torch.randint(0, len(im_dataset) - 1, (num_images,))\n",
    "ims = torch.cat([im_dataset[idx][None, :] for idx in idxs]).float()\n",
    "ims = ims.to(device)\n",
    "\n",
    "model = VQVAE().to(device)\n",
    "model.load_state_dict(torch.load(\"../vqvaeCeleb/vqvae_autoencoder.pth\", map_location = device))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    encoded_output, _ = model.encode(ims)\n",
    "    decoded_output = model.decode(encoded_output)\n",
    "    encoded_output = torch.clamp(encoded_output, -1., 1.)\n",
    "    encoded_output = (encoded_output + 1) / 2\n",
    "    decoded_output = torch.clamp(decoded_output, -1., 1.)\n",
    "    decoded_output = (decoded_output + 1) / 2\n",
    "    ims = (ims + 1) / 2\n",
    "\n",
    "    encoder_grid = vutils.make_grid(encoded_output.cpu(), nrow=ngrid)\n",
    "    decoder_grid = vutils.make_grid(decoded_output.cpu(), nrow=ngrid)\n",
    "    input_grid = vutils.make_grid(ims.cpu(), nrow=ngrid)\n",
    "    encoder_grid = transforms.ToPILImage()(encoder_grid)\n",
    "    decoder_grid = transforms.ToPILImage()(decoder_grid)\n",
    "    input_grid = transforms.ToPILImage()(input_grid)\n",
    "    \n",
    "    input_grid.save('../CelebAHQ/input_samples.png')\n",
    "    encoder_grid.save('../CelebAHQ/encoded_samples.png')\n",
    "    decoder_grid.save(\"../CelebAHQ/reconstructed_samples.png\")\n",
    "    \n",
    "    latent_path = \"../vqvaelatents\"\n",
    "    latent_fnames = glob.glob(os.path.join(\"../vqvaelatents\", '*.pkl'))\n",
    "    assert len(latent_fnames) == 0, 'Latents already present. Delete all latent files and re-run'\n",
    "    if not os.path.exists(latent_path):\n",
    "        os.mkdir(latent_path)\n",
    "    \n",
    "    fname_latent_map = {}\n",
    "    part_count = 0\n",
    "    count = 0\n",
    "    for idx, im in enumerate(tqdm(data_loader)):\n",
    "        encoded_output, _ = model.encode(im.float().to(device))\n",
    "        fname_latent_map[im_dataset.images[idx]] = encoded_output.cpu()\n",
    "        # Save latents every 1000 images\n",
    "        if (count+1) % 1000 == 0:\n",
    "            pickle.dump(fname_latent_map, open(os.path.join(latent_path,\n",
    "                                                            '{}.pkl'.format(part_count)), 'wb'))\n",
    "            part_count += 1\n",
    "            fname_latent_map = {}\n",
    "        count += 1\n",
    "    if len(fname_latent_map) > 0:\n",
    "        pickle.dump(fname_latent_map, open(os.path.join(latent_path,\n",
    "                                            '{}.pkl'.format(part_count)), 'wb'))\n",
    "    print(\"Done saving latents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Diffusion Model Architecture\n",
    "### UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, im_channels = 3):\n",
    "        super().__init__()\n",
    "        self.down_channels = [256, 384, 512, 768]\n",
    "        self.mid_channels = [768, 512]\n",
    "        self.t_emb_dim = 512\n",
    "        self.down_sample = [True, True, True]\n",
    "        self.num_down_layers = 2\n",
    "        self.num_mid_layers = 2\n",
    "        self.num_up_layers = 2\n",
    "        self.attns = [True, True, True]\n",
    "        self.norm_channels = 32\n",
    "        self.num_heads = 16\n",
    "        self.conv_out_channels = 128\n",
    "        \n",
    "        # Initial projection from sinusoidal time embedding\n",
    "        \n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
    "        )\n",
    "        \n",
    "        self.up_sample = list(reversed(self.down_sample))\n",
    "        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size = 3, padding = 1)\n",
    "        \n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels) - 1):\n",
    "            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i + 1],\n",
    "                                       t_emb_dim = self.t_emb_dim, down_sample = self.down_sample[i],\n",
    "                                       num_heads = self.num_heads, num_layers = self.num_down_layers,\n",
    "                                       attn = self.attns[i], norm_channels = self.norm_channels))\n",
    "        \n",
    "        self.mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels) - 1):\n",
    "            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1],\n",
    "                                    t_emb_dim = self.t_emb_dim, num_heads = self.num_heads,\n",
    "                                    num_layers = self.num_mid_layers, norm_channels = self.norm_channels))\n",
    "        \n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in reversed(range(len(self.down_channels) - 1)):\n",
    "            self.ups.append(UpBlockUnet(self.down_channels[i] * 2, self.down_channels[i - 1] if i != 0 else self.conv_out_channels,\n",
    "                                    self.t_emb_dim, up_sample = self.down_sample[i],\n",
    "                                        num_heads = self.num_heads,\n",
    "                                        num_layers = self.num_up_layers,\n",
    "                                        norm_channels = self.norm_channels))\n",
    "        \n",
    "        self.norm_out = nn.GroupNorm(self.norm_channels, self.conv_out_channels)\n",
    "        self.conv_out = nn.Conv2d(self.conv_out_channels, im_channels, kernel_size = 3, padding = 1)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # Shapes assuming downblocks are [C1, C2, C3, C4]\n",
    "        # Shapes assuming midblocks are [C4, C4, C3]\n",
    "        # Shapes assuming downsamples are [True, True, False]\n",
    "        # B x C x H x W\n",
    "        out = self.conv_in(x)\n",
    "        # B x C1 x H x W\n",
    "        \n",
    "        # t_emb -> B x t_emb_dim\n",
    "        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "        \n",
    "        down_outs = []\n",
    "        \n",
    "        for idx, down in enumerate(self.downs):\n",
    "            down_outs.append(out)\n",
    "            out = down(out, t_emb)\n",
    "        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n",
    "        # out B x C4 x H/4 x W/4\n",
    "        \n",
    "        for mid in self.mids:\n",
    "            out = mid(out, t_emb)\n",
    "        # out B x C3 x H/4 x W/4\n",
    "        \n",
    "        for up in self.ups:\n",
    "            down_out = down_outs.pop()\n",
    "            out = up(out, down_out, t_emb)\n",
    "            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n",
    "        out = self.norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.conv_out(out)\n",
    "        # out B x C x H x W\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Noise Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNoiseScheduler:\n",
    "    r\"\"\"\n",
    "    Class for the linear noise scheduler that is used in DDPM.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_timesteps, beta_start, beta_end):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        # Mimicking how compvis repo creates schedule\n",
    "        self.betas = (\n",
    "                torch.linspace(beta_start ** 0.5, beta_end ** 0.5, num_timesteps) ** 2\n",
    "        )\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
    "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n",
    "    \n",
    "    def add_noise(self, original, noise, t):\n",
    "        r\"\"\"\n",
    "        Forward method for diffusion\n",
    "        :param original: Image on which noise is to be applied\n",
    "        :param noise: Random Noise Tensor (from normal dist)\n",
    "        :param t: timestep of the forward process of shape -> (B,)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        original_shape = original.shape\n",
    "        batch_size = original_shape[0]\n",
    "        \n",
    "        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
    "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
    "        \n",
    "        # Reshape till (B,) becomes (B,1,1,1) if image is (B,C,H,W)\n",
    "        for _ in range(len(original_shape) - 1):\n",
    "            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n",
    "        for _ in range(len(original_shape) - 1):\n",
    "            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n",
    "        \n",
    "        # Apply and Return Forward process equation\n",
    "        return (sqrt_alpha_cum_prod.to(original.device) * original\n",
    "                + sqrt_one_minus_alpha_cum_prod.to(original.device) * noise)\n",
    "    \n",
    "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
    "        r\"\"\"\n",
    "            Use the noise prediction by model to get\n",
    "            xt-1 using xt and the nosie predicted\n",
    "        :param xt: current timestep sample\n",
    "        :param noise_pred: model noise prediction\n",
    "        :param t: current timestep we are at\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x0 = ((xt - (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t] * noise_pred)) /\n",
    "              torch.sqrt(self.alpha_cum_prod.to(xt.device)[t]))\n",
    "        x0 = torch.clamp(x0, -1., 1.)\n",
    "        \n",
    "        mean = xt - ((self.betas.to(xt.device)[t]) * noise_pred) / (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t])\n",
    "        mean = mean / torch.sqrt(self.alphas.to(xt.device)[t])\n",
    "        \n",
    "        if t == 0:\n",
    "            return mean, x0\n",
    "        else:\n",
    "            variance = (1 - self.alpha_cum_prod.to(xt.device)[t - 1]) / (1.0 - self.alpha_cum_prod.to(xt.device)[t])\n",
    "            variance = variance * self.betas.to(xt.device)[t]\n",
    "            sigma = variance ** 0.5\n",
    "            z = torch.randn(xt.shape).to(xt.device)\n",
    "            \n",
    "            # OR\n",
    "            # variance = self.betas[t]\n",
    "            # sigma = variance ** 0.5\n",
    "            # z = torch.randn(xt.shape).to(xt.device)\n",
    "            return mean + sigma * z, x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Diffusion Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Parameters\n",
    "lr = 5e-5\n",
    "\n",
    "# Diffusion Parameters\n",
    "beta_start = 1e-4\n",
    "beta_end = 2e-2\n",
    "T = 1000\n",
    "\n",
    "# Model Parameters\n",
    "nc = 3\n",
    "image_size = 256\n",
    "z_channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Latents for setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dataset = CelebDataset(split='train',\n",
    "                                im_path=path,\n",
    "                                im_size=image_size,\n",
    "                                im_channels=nc,\n",
    "                                use_latents=True,\n",
    "                                latent_path=\"../vqvaelatents\"\n",
    "                                )\n",
    "    \n",
    "celebALoader = DataLoader(im_dataset,\n",
    "                            batch_size=batch_size_ldm,\n",
    "                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = LinearNoiseScheduler(num_timesteps = T, beta_start = beta_start, beta_end = beta_end)\n",
    "\n",
    "model = Unet(im_channels = z_channels).to(device)\n",
    "model.train()\n",
    "\n",
    "if not im_dataset.use_latents:\n",
    "    vae = VQVAE().to(device)\n",
    "    vae.eval()\n",
    "    vae.load_state_dict(torch.load(\"../vqvaeCeleb/vqvae_autoencoder.pth\", map_location = device))\n",
    "\n",
    "if not im_dataset.use_latents:\n",
    "    for param in vae.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch_idx in range(num_epochs_ldm):\n",
    "    losses = []\n",
    "    for i, (im, _) in enumerate(celebALoader):\n",
    "        optimizer.zero_grad()\n",
    "        im = im.float().to(device)\n",
    "        \n",
    "        # THE MAIN PART -> LATENT SPACE TRAINING\n",
    "        if not im_dataset.use_latents:\n",
    "            with torch.no_grad():\n",
    "                im, _ = vae.encode(im)\n",
    "        \n",
    "        noise = torch.randn_like(im).to(device)\n",
    "        t = torch.randint(0, T, (im.shape[0],)).to(device)\n",
    "        \n",
    "        noisy_im = scheduler.add_noise(im, noise, t)\n",
    "        noise_pred = model(noisy_im, t)\n",
    "        \n",
    "        loss = criterion(noise_pred, noise)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch_idx + 1}/{num_epochs_ldm}], Loss: {np.mean(losses)}\")\n",
    "    torch.save(model.state_dict(), \"../ldmCeleb/denoiseLatentModelCeleb.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_grid_rows = 4\n",
    "num_samples = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = LinearNoiseScheduler(T, beta_start, beta_end)\n",
    "\n",
    "model = Unet(im_channels = z_channels).to(device)\n",
    "model.load_state_dict(torch.load(\"../ldmCeleb/denoiseLatentModelCeleb.pth\", map_location = device))\n",
    "model.eval()\n",
    "\n",
    "vae = VQVAE().to(device)\n",
    "vae.eval()\n",
    "vae.load_state_dict(torch.load(\"../vqvaeCeleb/vqvae_autoencoder.pth\", map_location=device), strict=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    im_size = image_size // (2 ** (sum(model.down_sample)))\n",
    "    xt = torch.randn((num_samples, z_channels, im_size, im_size)).to(device)\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        noise_pred = model(xt, torch.as_tensor(t).unsqueeze(0).to(device))\n",
    "        xt, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(t).to(device))\n",
    "        \n",
    "        ims_raw = torch.clamp(xt, -1., 1.).detach().cpu()\n",
    "        ims_raw = (ims_raw + 1) / 2\n",
    "        \n",
    "        ims = vae.decode(xt)\n",
    "        ims = torch.clamp(ims, -1., 1.).detach().cpu()\n",
    "        ims = (ims + 1) / 2\n",
    "        \n",
    "        grid_latent = vutils.make_grid(ims_raw, nrow = num_grid_rows, normalize = True)\n",
    "        grid_reconstructed = vutils.make_grid(ims, nrow = num_grid_rows, normalize = True)\n",
    "        \n",
    "        if (t % 50 == 0):\n",
    "            plt.figure(figsize = (15, 15))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(np.transpose(grid_latent.cpu().detach().numpy(), (1, 2, 0)))\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(np.transpose(grid_reconstructed.cpu().detach().numpy(), (1, 2, 0)))\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "        img_latent = transforms.ToPILImage()(grid_latent)\n",
    "        img_decode = transforms.ToPILImage()(grid_reconstructed)\n",
    "        if not os.path.exists(\"./DDPM/CelebLatent\"):\n",
    "            os.makedirs(\"./DDPM/CelebLatent\")\n",
    "            \n",
    "        if not os.path.exists(\"../DDPM/CelebDecode\"):\n",
    "            os.makedirs(\"../DDPM/CelebDecode\")\n",
    "        \n",
    "\n",
    "        img_latent.save(f\"../DDPM/CelebLatent/x0_{t}.png\")\n",
    "        img_latent.close()\n",
    "        \n",
    "        img_decode.save(f\"../DDPM/CelebDecode/x0_{t}.png\")\n",
    "        img_decode.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
