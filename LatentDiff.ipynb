{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Parameters\n",
    "batch_size = 64\n",
    "num_epochs = 40\n",
    "lr = 1e-4\n",
    "num_grid_rows = 8\n",
    "num_samples = 64\n",
    "\n",
    "# Diffusion Parameters\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "T = 1000\n",
    "\n",
    "# Model Parameters\n",
    "nc = 3\n",
    "image_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize([0.5 for _ in range(nc)], [0.5 for _ in range(nc)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "celebADataset = datasets.ImageFolder(root = \"../CelebA\", transform = transform)\n",
    "celebALoader = DataLoader(dataset = celebADataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "mnist_train = datasets.MNIST(root = \"../\", train = True, transform = transform, download = True)\n",
    "mnist_test = datasets.MNIST(root = \"../\", train = False, transform = transform, download = True)\n",
    "\n",
    "mnist_combined_loader = DataLoader(dataset = mnist_train + mnist_test, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Latent Diffusion Models**\n",
    "\n",
    "### The idea is to train the diffusion models on a low dimensional latent representation rather than the entire big pixel space. In addition to that, also train an Encoder-Decoder model that takes the original image converts it into the latent representation using the encoder and reconverts the latent representation to the reconstructed image.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Latent2.png\" style=\"width:60%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "### The downside is that although the $L1/L2$ reconstruction loss might be low, the perceptual features in the reconstructed image still might be **fuzzy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptual Retention & $\\text{LPIPS}$ as the metric\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Percept.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "CLearly as said that although the $L_1$ or $L_2$ reconstruction loss might be low for the image, yet the perceptual features in the image perceived by a human are still **blurry**. Now in order to understand how a model would perceive the image, there is no better place to dig into pretrained classification **CNNs** $\\to$ **VGGs**. The goal is to bring the feature map extracted at each VGG layer to be very similar to the original image's feature maps at each VGG layer. This distance metric between the feature maps extracted from the layers of a pretrained VGG is called the **perceptual loss**.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/LPIPS1.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "Check out the original implementation too at [Perceptual Similarity](https://github.com/richzhang/PerceptualSimilarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lpips.py implementation\n",
    "from collections import namedtuple\n",
    "\n",
    "def spatial_average(in_tens, keepdim = True):\n",
    "    return in_tens.mean([2, 3], keepdim = keepdim)\n",
    "\n",
    "\n",
    "class vgg16(nn.Module):\n",
    "    def __init__(self, requires_grad = False, pretrained = True):\n",
    "        super().__init__()\n",
    "        vgg_pretrained_features = models.vgg16(pretrained = pretrained).features\n",
    "        self.slice1 = nn.Sequential()\n",
    "        self.slice2 = nn.Sequential()\n",
    "        self.slice3 = nn.Sequential()\n",
    "        self.slice4 = nn.Sequential()\n",
    "        self.slice5 = nn.Sequential()\n",
    "        self.N_slices = 5\n",
    "        for x in range(4):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "            \n",
    "        for x in range(4, 9):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        for x in range(9, 16):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        for x in range(16, 23):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        for x in range(23, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        # Freeze the model\n",
    "        if requires_grad == False:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu1_2 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2_2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3_3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4_3 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5_3 = h\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\", \"relu5_3\"])\n",
    "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ScalingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # mean = [0.485, 0.456, 0.406]\n",
    "        # std = [0.229, 0.224, 0.225]\n",
    "        self.register_buffer(\"shift\", torch.tensor([-.030, -.088, -.188])[None, :, None, None])\n",
    "        self.register_buffer(\"scale\", torch.tensor([.458, .448, .450])[None, :, None, None])\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return (inp - self.shift) / self.scale\n",
    "\n",
    "class NetLinLayer(nn.Module):\n",
    "    def __init__(self, chn_in, chn_out = 1, use_dropout = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.Dropout(), ] if (use_dropout) else []\n",
    "        layers += [nn.Conv2d(chn_in, chn_out, kernel_size = 1, stride = 1, padding = 0, bias = False), ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class LPIPS(nn.Module):\n",
    "    def __init__(self, net = \"vgg\", version = \"0.1\", use_dropout = True):\n",
    "        super().__init__()\n",
    "        self.scaling_layer = ScalingLayer()\n",
    "        \n",
    "        self.chns = [64, 128, 256, 512, 512]\n",
    "        self.L = len(self.chns)\n",
    "        self.net = vgg16(pretrained = True, requires_grad = False)\n",
    "        \n",
    "        self.lin0 = NetLinLayer(self.chns[0], use_dropout = use_dropout)\n",
    "        self.lin1 = NetLinLayer(self.chns[1], use_dropout = use_dropout)\n",
    "        self.lin2 = NetLinLayer(self.chns[2], use_dropout = use_dropout)\n",
    "        self.lin3 = NetLinLayer(self.chns[3], use_dropout = use_dropout)\n",
    "        self.lin4 = NetLinLayer(self.chns[4], use_dropout = use_dropout)\n",
    "        self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n",
    "        self.lins = nn.ModuleList(self.lins)\n",
    "        \n",
    "        import inspect\n",
    "        \n",
    "        model_path = os.path.abspath(\n",
    "            os.path.join(inspect.getfile(self.__init__), \"..\", \"weights/v%s/%s.pth\" % (version, net))\n",
    "        )\n",
    "        print(\"Loading model from: %s\" % model_path)\n",
    "        self.net.load_state_dict(torch.load(model_path, map_location = device), strict = False)\n",
    "        \n",
    "        self.eval()\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    \n",
    "    def forward(self, in0, in1, normalize = False):\n",
    "        if normalize:\n",
    "            in0 = 2 * in0 - 1\n",
    "            in1 = 2 * in1 - 1\n",
    "        \n",
    "        in0_input, in1_input = self.scaling_layer(in0), self.scaling_layer(in1)\n",
    "        \n",
    "        outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)\n",
    "        \n",
    "        feats0, feats1, diffs = {}, {}, {}\n",
    "        \n",
    "        for kk in range(self.L):\n",
    "            feats0[kk], feats1[kk] = F.normalize(outs0[kk], dim = 1), F.normalize(outs1[kk])\n",
    "            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n",
    "        \n",
    "        res = [spatial_average(self.lins[kk](diffs[kk]), keepdim = True) for kk in range(self.L)]\n",
    "        val = 0\n",
    "        \n",
    "        for l in range(self.L):\n",
    "            val += res[l]\n",
    "        \n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizing the Latent Space using the $\\text{CodeBooks}$ from $\\text{VQVAEs}$\n",
    "### $\\text{VQVAE}$ as the $\\text{AutoEncoder}$\n",
    "\n",
    "$k$ vectors, each of $d$ dimensions $(k \\times d)$ help us encode the data.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE1.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "The encoder generates a feature map of $H \\times W$ features each of $d$ dimension.\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE2.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "For each of the features, we find the nearest $d$ dimensional encoding to it and replace it with that.\n",
    "\n",
    "$$ z_q(x) = e_k $$\n",
    "$$ k = \\argmin_j || z_e(x) - e_j ||_2 $$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE3.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "The decoder then discards off the feature map given by the encoder and only uses the nearest codeblock feature map to reconstruct the output image.\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE4.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "The issue is we have to define the gradients for the $\\argmin$ step separately for the gradients to flow back. We approximate the gradient similar to the straight-through estimator and just copy gradients from decoder input $z_q(x)$ to encoder output $z_e(x)$\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE5.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "$$ L = \\log p(x | z_q(x)) + || \\text{sg}[z_e(x)] - e ||_2^2 + \\beta || z_e(x) - \\text{sg}[e] ||_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The AutoEncoder Architecture\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/AutoEnc1.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/AutoEnc2.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/AutoEnc3.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/AutoEnc4.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model Blocks\n",
    "Adapted from [ExplainingAI](https://github.com/explainingai-code/StableDiffusion-PyTorch/blob/main/models/blocks.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Embedding\n",
    "\n",
    "def get_time_embedding(time_steps, temb_dim):\n",
    "    factor = 10000 ** ((torch.arange(\n",
    "        start = 0, end = temb_dim // 2, dtype = torch.float32, device = time_steps.device) / (temb_dim // 2))\n",
    "    )\n",
    "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim = -1)\n",
    "    return t_emb\n",
    "\n",
    "\n",
    "# Model Blocks\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Down conv block with attention.\n",
    "    Sequence of following block\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Downsample\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 down_sample, num_heads, num_layers, attn, norm_channels, cross_attn=False, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.down_sample = down_sample\n",
    "        self.attn = attn\n",
    "        self.context_dim = context_dim\n",
    "        self.cross_attn = cross_attn\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(self.t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.attn:\n",
    "            self.attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            \n",
    "            self.attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "        \n",
    "        if self.cross_attn:\n",
    "            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n",
    "            self.cross_attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.cross_attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.context_proj = nn.ModuleList(\n",
    "                [nn.Linear(context_dim, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n",
    "                                          4, 2, 1) if self.down_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t_emb=None, context=None):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet block of Unet\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            if self.attn:\n",
    "                # Attention block of Unet\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "            \n",
    "            if self.cross_attn:\n",
    "                assert context is not None, \"context cannot be None if cross attention layers are used\"\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.cross_attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n",
    "                context_proj = self.context_proj[i](context)\n",
    "                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "            \n",
    "        # Downsample\n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MidBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Mid conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Resnet block with time embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads, num_layers, norm_channels, cross_attn=None, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.context_dim = context_dim\n",
    "        self.cross_attn = cross_attn\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers + 1)\n",
    "            ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(norm_channels, out_channels)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.attentions = nn.ModuleList(\n",
    "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        if self.cross_attn:\n",
    "            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n",
    "            self.cross_attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.cross_attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.context_proj = nn.ModuleList(\n",
    "                [nn.Linear(context_dim, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t_emb=None, context=None):\n",
    "        out = x\n",
    "        \n",
    "        # First resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        if self.t_emb_dim is not None:\n",
    "            out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.residual_input_conv[0](resnet_input)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            # Attention Block\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "            \n",
    "            if self.cross_attn:\n",
    "                assert context is not None, \"context cannot be None if cross attention layers are used\"\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.cross_attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n",
    "                context_proj = self.context_proj[i](context)\n",
    "                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "                \n",
    "            \n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i + 1](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i + 1](out)\n",
    "            out = out + self.residual_input_conv[i + 1](resnet_input)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Up conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Upsample\n",
    "    1. Concatenate Down block output\n",
    "    2. Resnet block with time embedding\n",
    "    3. Attention Block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 up_sample, num_heads, num_layers, attn, norm_channels):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.attn = attn\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "        \n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        if self.attn:\n",
    "            self.attention_norms = nn.ModuleList(\n",
    "                [\n",
    "                    nn.GroupNorm(norm_channels, out_channels)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            self.attentions = nn.ModuleList(\n",
    "                [\n",
    "                    nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(in_channels, in_channels,\n",
    "                                                 4, 2, 1) \\\n",
    "            if self.up_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, out_down=None, t_emb=None):\n",
    "        # Upsample\n",
    "        x = self.up_sample_conv(x)\n",
    "        \n",
    "        # Concat with Downblock output\n",
    "        if out_down is not None:\n",
    "            x = torch.cat([x, out_down], dim=1)\n",
    "        \n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            # Self Attention\n",
    "            if self.attn:\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpBlockUnet(nn.Module):\n",
    "    r\"\"\"\n",
    "    Up conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Upsample\n",
    "    1. Concatenate Down block output\n",
    "    2. Resnet block with time embedding\n",
    "    3. Attention Block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample,\n",
    "                 num_heads, num_layers, norm_channels, cross_attn=False, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.cross_attn = cross_attn\n",
    "        self.context_dim = context_dim\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "            \n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [\n",
    "                nn.GroupNorm(norm_channels, out_channels)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attentions = nn.ModuleList(\n",
    "            [\n",
    "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.cross_attn:\n",
    "            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n",
    "            self.cross_attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.cross_attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.context_proj = nn.ModuleList(\n",
    "                [nn.Linear(context_dim, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n",
    "                                                 4, 2, 1) \\\n",
    "            if self.up_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, out_down=None, t_emb=None, context=None):\n",
    "        x = self.up_sample_conv(x)\n",
    "        if out_down is not None:\n",
    "            x = torch.cat([x, out_down], dim=1)\n",
    "        \n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            # Self Attention\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "            # Cross Attention\n",
    "            if self.cross_attn:\n",
    "                assert context is not None, \"context cannot be None if cross attention layers are used\"\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.cross_attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                assert len(context.shape) == 3, \\\n",
    "                    \"Context shape does not match B,_,CONTEXT_DIM\"\n",
    "                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim,\\\n",
    "                    \"Context shape does not match B,_,CONTEXT_DIM\"\n",
    "                context_proj = self.context_proj[i](context)\n",
    "                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQVAE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        im_channels = 3\n",
    "        self.down_channels = [64, 128, 256, 256]\n",
    "        self.mid_channels = [256, 256]\n",
    "        self.down_sample = [True, True, True]\n",
    "        self.num_down_layers = 2\n",
    "        self.num_mid_layers = 2\n",
    "        self.num_up_layers = 2\n",
    "        self.norm_channels = 32\n",
    "        \n",
    "        # To disable attention in the DownBlock of Encoder and UpBlock of Decoder\n",
    "        self.attns = [False, False, False]\n",
    "        self.num_heads = 4\n",
    "        \n",
    "        # Latent Dimension\n",
    "        self.z_channels = 3\n",
    "        self.codebook_size = 8192\n",
    "        \n",
    "        self.up_sample = list(reversed(self.down_sample))\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size = 3, stride = 1, padding = (1, 1))\n",
    "        \n",
    "        # DownBlock + MidBlock\n",
    "        \n",
    "        self.encoder_downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels) - 1):\n",
    "            self.encoder_downs.append(DownBlock(self.down_channels[i], self.down_channels[i + 1],\n",
    "                                                t_emb_dim = None, down_sample = self.down_sample[i],\n",
    "                                                num_heads = self.num_heads, num_layers = self.num_down_layers,\n",
    "                                                attn = self.attns[i], norm_channels = self.norm_channels))\n",
    "            \n",
    "            \n",
    "        self.encoder_mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels) - 1):\n",
    "            self.encoder_mids.append(MidBlock(self.down_channels[i], self.mid_channels[i + 1],\n",
    "                                             t_emb_dim = None, num_heads = self.num_heads,\n",
    "                                             num_layers = self.num_mid_layers, norm_channels = self.norm_channels))\n",
    "        \n",
    "        self.encoder_norm_out = nn.GroupNorm(self.norm_channels, self.mid_channels[-1])\n",
    "        self.encoder_conv_out = nn.Conv2d(self.mid_channels[-1], self.z_channels, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        # Pre-Quantization Convolution\n",
    "        self.pre_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size = 1)\n",
    "        \n",
    "        # Codebook\n",
    "        self.embedding = nn.Embedding(self.codebook_size, self.z_channels)\n",
    "        \n",
    "        \n",
    "        # Decoder\n",
    "        # Post Quantization Convolution\n",
    "        self.post_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size = 1)\n",
    "        self.decoder_conv_in = nn.Conv2d(self.z_channels, self.mid_channels[-1], kernel_size = 3, padding = (1, 1))\n",
    "        \n",
    "        # MidBlock + UpBlock\n",
    "        self.decoder_mids = nn.ModuleList([])\n",
    "        for i in reversed(range(1, len(self.mid_channels))):\n",
    "            self.decoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i - 1],\n",
    "                                             t_emb_dim = None, num_heads = self.num_heads,\n",
    "                                             num_layers = self.num_mid_layers, norm_channels = self.norm_channels))\n",
    "        \n",
    "        self.decoder_ups = nn.ModuleList([])\n",
    "        for i in reversed(range(1, len(self.down_channels))):\n",
    "            self.decoder_ups.append(UpBlock(self.down_channels[i], self.down_channels[i - 1],\n",
    "                                           t_emb_dim = None, up_sample = self.down_sample[i - 1],\n",
    "                                           num_heads = self.num_heads, num_layers = self.num_up_layers,\n",
    "                                           attn = self.attns[i - 1], norm_channels = self.norm_channels))\n",
    "        \n",
    "        self.decoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[0])\n",
    "        self.decoder_conv_out = nn.Conv2d(self.down_channels[0], im_channels, kernel_size = 3, padding = 1)\n",
    "    \n",
    "    def quantize(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = x.reshape(x.size(0), -1, x.size(-1))\n",
    "        \n",
    "        # Nearest Embedding in the Codebook\n",
    "        # dist between (B, H*W, C) and (B, K, C) -> (B, H*W, K)\n",
    "        dist = torch.cdist(x, self.embedding.weight[None, :].repeat((x.size(0), 1, 1)))\n",
    "        \n",
    "        # (B, H*W)\n",
    "        min_encoding_indices = torch.argmin(dist, dim = -1)\n",
    "        \n",
    "        # Replace encoder output with nearest embedding\n",
    "        # quant_out -> B*H*W, C\n",
    "        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1))\n",
    "        \n",
    "        # x -> B*H*W, C\n",
    "        x = x.reshape((-1, x.size(-1)))\n",
    "        commitment_loss = torch.mean((quant_out.detach() - x) ** 2)\n",
    "        # codebook_loss\n",
    "    \n",
    "    def encode(self, x):\n",
    "        out = self.encoder_conv_in(x)\n",
    "        for idx, down in enumerate(self.encoder_downs):\n",
    "            out = down(out)\n",
    "        \n",
    "        for mid in self.encoder_mids:\n",
    "            out = mid(out)\n",
    "        \n",
    "        out = self.encoder_norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.encoder_conv_out(out)\n",
    "        out = self.pre_quant_conv(out)\n",
    "        out, quant_losses = self.quantize(out)\n",
    "        return out, quant_losses\n",
    "    \n",
    "    def decode(self, z):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z, quant_losses = self.encode(x)\n",
    "        out = self.decode(z)\n",
    "        return out, z, quant_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"./Media/Latent3.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Latent4.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
