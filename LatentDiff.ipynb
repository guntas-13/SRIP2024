{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Latent Diffusion Models**\n",
    "\n",
    "### The idea is to train the diffusion models on a low dimensional latent representation rather than the entire big pixel space. In addition to that, also train an Encoder-Decoder model that takes the original image converts it into the latent representation using the encoder and reconverts the latent representation to the reconstructed image.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Latent2.png\" style=\"width:60%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "### The downside is that although the $L1/L2$ reconstruction loss might be low, the perceptual features in the reconstructed image still might be **fuzzy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptual Retention & $\\text{LPIPS}$ as the metric\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Percept.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "CLearly as said that although the $L_1$ or $L_2$ reconstruction loss might be low for the image, yet the perceptual features in the image perceived by a human are still **blurry**. Now in order to understand how a model would perceive the image, there is no better place to dig into pretrained classification **CNNs** $\\to$ **VGGs**. The goal is to bring the feature map extracted at each VGG layer to be very similar to the original image's feature maps at each VGG layer. This distance metric between the feature maps extracted from the layers of a pretrained VGG is called the **perceptual loss**.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/LPIPS1.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "Check out the original implementation too at [Perceptual Similarity](https://github.com/richzhang/PerceptualSimilarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lpips.py implementation\n",
    "from collections import namedtuple\n",
    "\n",
    "def spatial_average(in_tens, keepdim = True):\n",
    "    return in_tens.mean([2, 3], keepdim = keepdim)\n",
    "\n",
    "\n",
    "class vgg16(nn.Module):\n",
    "    def __init__(self, requires_grad = False, pretrained = True):\n",
    "        super().__init__()\n",
    "        vgg_pretrained_features = models.vgg16(pretrained = pretrained).features\n",
    "        self.slice1 = nn.Sequential()\n",
    "        self.slice2 = nn.Sequential()\n",
    "        self.slice3 = nn.Sequential()\n",
    "        self.slice4 = nn.Sequential()\n",
    "        self.slice5 = nn.Sequential()\n",
    "        self.N_slices = 5\n",
    "        for x in range(4):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "            \n",
    "        for x in range(4, 9):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        for x in range(9, 16):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        for x in range(16, 23):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        for x in range(23, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        \n",
    "        # Freeze the model\n",
    "        if requires_grad == False:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, X):\n",
    "        h = self.slice1(X)\n",
    "        h_relu1_2 = h\n",
    "        h = self.slice2(h)\n",
    "        h_relu2_2 = h\n",
    "        h = self.slice3(h)\n",
    "        h_relu3_3 = h\n",
    "        h = self.slice4(h)\n",
    "        h_relu4_3 = h\n",
    "        h = self.slice5(h)\n",
    "        h_relu5_3 = h\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\", \"relu5_3\"])\n",
    "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ScalingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # mean = [0.485, 0.456, 0.406]\n",
    "        # std = [0.229, 0.224, 0.225]\n",
    "        self.register_buffer(\"shift\", torch.tensor([-.030, -.088, -.188])[None, :, None, None])\n",
    "        self.register_buffer(\"scale\", torch.tensor([.458, .448, .450])[None, :, None, None])\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return (inp - self.shift) / self.scale\n",
    "\n",
    "class NetLinLayer(nn.Module):\n",
    "    def __init__(self, chn_in, chn_out = 1, use_dropout = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.Dropout(), ] if (use_dropout) else []\n",
    "        layers += [nn.Conv2d(chn_in, chn_out, kernel_size = 1, stride = 1, padding = 0, bias = False), ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class LPIPS(nn.Module):\n",
    "    def __init__(self, net = \"vgg\", version = \"0.1\", use_dropout = True):\n",
    "        super().__init__()\n",
    "        self.scaling_layer = ScalingLayer()\n",
    "        \n",
    "        self.chns = [64, 128, 256, 512, 512]\n",
    "        self.L = len(self.chns)\n",
    "        self.net = vgg16(pretrained = True, requires_grad = False)\n",
    "        \n",
    "        self.lin0 = NetLinLayer(self.chns[0], use_dropout = use_dropout)\n",
    "        self.lin1 = NetLinLayer(self.chns[1], use_dropout = use_dropout)\n",
    "        self.lin2 = NetLinLayer(self.chns[2], use_dropout = use_dropout)\n",
    "        self.lin3 = NetLinLayer(self.chns[3], use_dropout = use_dropout)\n",
    "        self.lin4 = NetLinLayer(self.chns[4], use_dropout = use_dropout)\n",
    "        self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n",
    "        self.lins = nn.ModuleList(self.lins)\n",
    "        \n",
    "        # import inspect\n",
    "        \n",
    "        # model_path = os.path.abspath(\n",
    "        #     os.path.join(inspect.getfile(self.__init__), \"..\", \"weights/v%s/%s.pth\" % (version, net))\n",
    "        # )\n",
    "        # print(\"Loading model from: %s\" % model_path)\n",
    "        self.net.load_state_dict(torch.load(\"./vgg.pth\", map_location = device), strict = False)\n",
    "        \n",
    "        self.eval()\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    \n",
    "    def forward(self, in0, in1, normalize = False):\n",
    "        if normalize:\n",
    "            in0 = 2 * in0 - 1\n",
    "            in1 = 2 * in1 - 1\n",
    "        \n",
    "        in0_input, in1_input = self.scaling_layer(in0), self.scaling_layer(in1)\n",
    "        \n",
    "        outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)\n",
    "        \n",
    "        feats0, feats1, diffs = {}, {}, {}\n",
    "        \n",
    "        for kk in range(self.L):\n",
    "            feats0[kk], feats1[kk] = F.normalize(outs0[kk], dim = 1), F.normalize(outs1[kk])\n",
    "            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n",
    "        \n",
    "        res = [spatial_average(self.lins[kk](diffs[kk]), keepdim = True) for kk in range(self.L)]\n",
    "        val = 0\n",
    "        \n",
    "        for l in range(self.L):\n",
    "            val += res[l]\n",
    "        \n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizing the Latent Space using the $\\text{CodeBooks}$ from $\\text{VQVAEs}$\n",
    "### $\\text{VQVAE}$ as the $\\text{AutoEncoder}$\n",
    "\n",
    "$k$ vectors, each of $d$ dimensions $(k \\times d)$ help us encode the data.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE1.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "The encoder generates a feature map of $H \\times W$ features each of $d$ dimension.\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE2.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "For each of the features, we find the nearest $d$ dimensional encoding to it and replace it with that.\n",
    "\n",
    "$$ z_q(x) = e_k $$\n",
    "$$ k = \\argmin_j || z_e(x) - e_j ||_2 $$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE3.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "The decoder then discards off the feature map given by the encoder and only uses the nearest codeblock feature map to reconstruct the output image.\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE4.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "The issue is we have to define the gradients for the $\\argmin$ step separately for the gradients to flow back. We approximate the gradient similar to the straight-through estimator and just copy gradients from decoder input $z_q(x)$ to encoder output $z_e(x)$\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/VQVAE5.png\" style=\"width:70%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "$$ L = \\log p(x | z_q(x)) + || \\text{sg}[z_e(x)] - e ||_2^2 + \\beta || z_e(x) - \\text{sg}[e] ||_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The AutoEncoder Architecture\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Latent3.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/Latent4.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/AutoEnc1.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/AutoEnc2.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/AutoEnc3.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./Media/AutoEnc4.png\" style=\"width:80%;border:0;\" alt = \"image\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model Blocks\n",
    "Adapted from [ExplainingAI](https://github.com/explainingai-code/StableDiffusion-PyTorch/blob/main/models/blocks.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Embedding\n",
    "\n",
    "def get_time_embedding(T, d_model):\n",
    "    factor = 10000 ** ((torch.arange(start = 0, end = d_model // 2, dtype = torch.float32, device = T.device)) / (d_model // 2))\n",
    "    t_emb = T[:, None].repeat(1, d_model // 2) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim = -1)\n",
    "    return t_emb\n",
    "\n",
    "\n",
    "# Model Blocks\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Down conv block with attention.\n",
    "    Sequence of following block\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Downsample\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 down_sample, num_heads, num_layers, attn, norm_channels, cross_attn=False, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.down_sample = down_sample\n",
    "        self.attn = attn\n",
    "        self.context_dim = context_dim\n",
    "        self.cross_attn = cross_attn\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(self.t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.attn:\n",
    "            self.attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            \n",
    "            self.attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "        \n",
    "        if self.cross_attn:\n",
    "            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n",
    "            self.cross_attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.cross_attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.context_proj = nn.ModuleList(\n",
    "                [nn.Linear(context_dim, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n",
    "                                          4, 2, 1) if self.down_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t_emb=None, context=None):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet block of Unet\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            if self.attn:\n",
    "                # Attention block of Unet\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "            \n",
    "            if self.cross_attn:\n",
    "                assert context is not None, \"context cannot be None if cross attention layers are used\"\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.cross_attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n",
    "                context_proj = self.context_proj[i](context)\n",
    "                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "            \n",
    "        # Downsample\n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MidBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Mid conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Resnet block with time embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads, num_layers, norm_channels, cross_attn=None, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.context_dim = context_dim\n",
    "        self.cross_attn = cross_attn\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers + 1)\n",
    "            ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(norm_channels, out_channels)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.attentions = nn.ModuleList(\n",
    "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        if self.cross_attn:\n",
    "            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n",
    "            self.cross_attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.cross_attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.context_proj = nn.ModuleList(\n",
    "                [nn.Linear(context_dim, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t_emb=None, context=None):\n",
    "        out = x\n",
    "        \n",
    "        # First resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        if self.t_emb_dim is not None:\n",
    "            out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.residual_input_conv[0](resnet_input)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            # Attention Block\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "            \n",
    "            if self.cross_attn:\n",
    "                assert context is not None, \"context cannot be None if cross attention layers are used\"\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.cross_attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n",
    "                context_proj = self.context_proj[i](context)\n",
    "                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "                \n",
    "            \n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i + 1](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i + 1](out)\n",
    "            out = out + self.residual_input_conv[i + 1](resnet_input)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Up conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Upsample\n",
    "    1. Concatenate Down block output\n",
    "    2. Resnet block with time embedding\n",
    "    3. Attention Block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 up_sample, num_heads, num_layers, attn, norm_channels):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.attn = attn\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "        \n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        if self.attn:\n",
    "            self.attention_norms = nn.ModuleList(\n",
    "                [\n",
    "                    nn.GroupNorm(norm_channels, out_channels)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            self.attentions = nn.ModuleList(\n",
    "                [\n",
    "                    nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(in_channels, in_channels,\n",
    "                                                 4, 2, 1) \\\n",
    "            if self.up_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, out_down=None, t_emb=None):\n",
    "        # Upsample\n",
    "        x = self.up_sample_conv(x)\n",
    "        \n",
    "        # Concat with Downblock output\n",
    "        if out_down is not None:\n",
    "            x = torch.cat([x, out_down], dim=1)\n",
    "        \n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            # Self Attention\n",
    "            if self.attn:\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpBlockUnet(nn.Module):\n",
    "    r\"\"\"\n",
    "    Up conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Upsample\n",
    "    1. Concatenate Down block output\n",
    "    2. Resnet block with time embedding\n",
    "    3. Attention Block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample,\n",
    "                 num_heads, num_layers, norm_channels, cross_attn=False, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.cross_attn = cross_attn\n",
    "        self.context_dim = context_dim\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "            \n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [\n",
    "                nn.GroupNorm(norm_channels, out_channels)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attentions = nn.ModuleList(\n",
    "            [\n",
    "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.cross_attn:\n",
    "            assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n",
    "            self.cross_attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.cross_attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            self.context_proj = nn.ModuleList(\n",
    "                [nn.Linear(context_dim, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n",
    "                                                 4, 2, 1) \\\n",
    "            if self.up_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, out_down=None, t_emb=None, context=None):\n",
    "        x = self.up_sample_conv(x)\n",
    "        if out_down is not None:\n",
    "            x = torch.cat([x, out_down], dim=1)\n",
    "        \n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            # Self Attention\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "            # Cross Attention\n",
    "            if self.cross_attn:\n",
    "                assert context is not None, \"context cannot be None if cross attention layers are used\"\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.cross_attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                assert len(context.shape) == 3, \\\n",
    "                    \"Context shape does not match B,_,CONTEXT_DIM\"\n",
    "                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim,\\\n",
    "                    \"Context shape does not match B,_,CONTEXT_DIM\"\n",
    "                context_proj = self.context_proj[i](context)\n",
    "                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQVAE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        im_channels = 3\n",
    "        self.down_channels = [64, 128, 256, 256]\n",
    "        self.mid_channels = [256, 256]\n",
    "        self.down_sample = [True, True, True]\n",
    "        self.num_down_layers = 2\n",
    "        self.num_mid_layers = 2\n",
    "        self.num_up_layers = 2\n",
    "        self.norm_channels = 32\n",
    "        \n",
    "        # To disable attention in the DownBlock of Encoder and UpBlock of Decoder\n",
    "        self.attns = [False, False, False]\n",
    "        self.num_heads = 4\n",
    "        \n",
    "        # Latent Dimension\n",
    "        self.z_channels = 3\n",
    "        self.codebook_size = 8192\n",
    "        \n",
    "        self.up_sample = list(reversed(self.down_sample))\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size = 3, stride = 1, padding = (1, 1))\n",
    "        \n",
    "        # DownBlock + MidBlock\n",
    "        \n",
    "        self.encoder_downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels) - 1):\n",
    "            self.encoder_downs.append(DownBlock(self.down_channels[i], self.down_channels[i + 1],\n",
    "                                                t_emb_dim = None, down_sample = self.down_sample[i],\n",
    "                                                num_heads = self.num_heads, num_layers = self.num_down_layers,\n",
    "                                                attn = self.attns[i], norm_channels = self.norm_channels))\n",
    "            \n",
    "            \n",
    "        self.encoder_mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels) - 1):\n",
    "            self.encoder_mids.append(MidBlock(self.down_channels[i], self.mid_channels[i + 1],\n",
    "                                             t_emb_dim = None, num_heads = self.num_heads,\n",
    "                                             num_layers = self.num_mid_layers, norm_channels = self.norm_channels))\n",
    "        \n",
    "        self.encoder_norm_out = nn.GroupNorm(self.norm_channels, self.mid_channels[-1])\n",
    "        self.encoder_conv_out = nn.Conv2d(self.mid_channels[-1], self.z_channels, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        # Pre-Quantization Convolution\n",
    "        self.pre_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size = 1)\n",
    "        \n",
    "        # Codebook\n",
    "        self.embedding = nn.Embedding(self.codebook_size, self.z_channels)\n",
    "        \n",
    "        \n",
    "        # Decoder\n",
    "        # Post Quantization Convolution\n",
    "        self.post_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size = 1)\n",
    "        self.decoder_conv_in = nn.Conv2d(self.z_channels, self.mid_channels[-1], kernel_size = 3, padding = (1, 1))\n",
    "        \n",
    "        # MidBlock + UpBlock\n",
    "        self.decoder_mids = nn.ModuleList([])\n",
    "        for i in reversed(range(1, len(self.mid_channels))):\n",
    "            self.decoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i - 1],\n",
    "                                             t_emb_dim = None, num_heads = self.num_heads,\n",
    "                                             num_layers = self.num_mid_layers, norm_channels = self.norm_channels))\n",
    "        \n",
    "        self.decoder_ups = nn.ModuleList([])\n",
    "        for i in reversed(range(1, len(self.down_channels))):\n",
    "            self.decoder_ups.append(UpBlock(self.down_channels[i], self.down_channels[i - 1],\n",
    "                                           t_emb_dim = None, up_sample = self.down_sample[i - 1],\n",
    "                                           num_heads = self.num_heads, num_layers = self.num_up_layers,\n",
    "                                           attn = self.attns[i - 1], norm_channels = self.norm_channels))\n",
    "        \n",
    "        self.decoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[0])\n",
    "        self.decoder_conv_out = nn.Conv2d(self.down_channels[0], im_channels, kernel_size = 3, padding = 1)\n",
    "    \n",
    "    def quantize(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = x.reshape(x.size(0), -1, x.size(-1))\n",
    "        \n",
    "        # Nearest Embedding in the Codebook\n",
    "        # dist between (B, H*W, C) and (B, K, C) -> (B, H*W, K)\n",
    "        dist = torch.cdist(x, self.embedding.weight[None, :].repeat((x.size(0), 1, 1)))\n",
    "        \n",
    "        # (B, H*W)\n",
    "        min_encoding_indices = torch.argmin(dist, dim = -1)\n",
    "        \n",
    "        # Replace encoder output with nearest embedding\n",
    "        # quant_out -> B*H*W, C\n",
    "        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1))\n",
    "        \n",
    "        # x -> B*H*W, C\n",
    "        x = x.reshape((-1, x.size(-1)))\n",
    "        commitment_loss = torch.mean((quant_out.detach() - x) ** 2)\n",
    "        codebook_loss = torch.mean((quant_out - x.detach()) ** 2)\n",
    "        quantize_losses = {\n",
    "            \"codebook_loss\" : codebook_loss,\n",
    "            \"commitment_loss\" : commitment_loss\n",
    "        }\n",
    "        \n",
    "        quant_out = x + (quant_out - x).detach()\n",
    "        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2)\n",
    "        \n",
    "        return quant_out, quantize_losses\n",
    "        \n",
    "    \n",
    "    def encode(self, x):\n",
    "        out = self.encoder_conv_in(x)\n",
    "        for idx, down in enumerate(self.encoder_downs):\n",
    "            out = down(out)\n",
    "        \n",
    "        for mid in self.encoder_mids:\n",
    "            out = mid(out)\n",
    "        \n",
    "        out = self.encoder_norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.encoder_conv_out(out)\n",
    "        out = self.pre_quant_conv(out)\n",
    "        out, quant_losses = self.quantize(out)\n",
    "        return out, quant_losses\n",
    "    \n",
    "    def decode(self, z):\n",
    "        out = z\n",
    "        out = self.post_quant_conv(out)\n",
    "        out = self.decoder_conv_in(out)\n",
    "        for mid in self.decoder_mids:\n",
    "            out = mid(out)\n",
    "        \n",
    "        for up in self.decoder_ups:\n",
    "            out = up(out)\n",
    "            \n",
    "        out = self.decoder_norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.decoder_conv_out(out)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z, quant_losses = self.encode(x)\n",
    "        out = self.decode(z)\n",
    "        return out, z, quant_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    r\"\"\"\n",
    "    PatchGAN Discriminator.\n",
    "    Rather than taking IMG_CHANNELSxIMG_HxIMG_W all the way to\n",
    "    1 scalar value , we instead predict grid of values.\n",
    "    Where each grid is prediction of how likely\n",
    "    the discriminator thinks that the image patch corresponding\n",
    "    to the grid cell is real\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, im_channels=3,\n",
    "                 conv_channels=[64, 128, 256],\n",
    "                 kernels=[4,4,4,4],\n",
    "                 strides=[2,2,2,1],\n",
    "                 paddings=[1,1,1,1]):\n",
    "        super().__init__()\n",
    "        self.im_channels = im_channels\n",
    "        activation = nn.LeakyReLU(0.2)\n",
    "        layers_dim = [self.im_channels] + conv_channels + [1]\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(layers_dim[i], layers_dim[i + 1],\n",
    "                          kernel_size=kernels[i],\n",
    "                          stride=strides[i],\n",
    "                          padding=paddings[i],\n",
    "                          bias=False if i !=0 else True),\n",
    "                nn.BatchNorm2d(layers_dim[i + 1]) if i != len(layers_dim) - 2 and i != 0 else nn.Identity(),\n",
    "                activation if i != len(layers_dim) - 2 else nn.Identity()\n",
    "            )\n",
    "            for i in range(len(layers_dim) - 1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Parameters\n",
    "batch_size = 16\n",
    "num_epochs = 20\n",
    "\n",
    "# Model Parameters\n",
    "nc = 3\n",
    "image_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize([0.5 for _ in range(nc)], [0.5 for _ in range(nc)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celebADataset = datasets.ImageFolder(root = \"../CelebA\", transform = transform)\n",
    "celebALoader = DataLoader(dataset = celebADataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "mnist_train = datasets.MNIST(root = \"../\", train = True, transform = transform, download = True)\n",
    "mnist_test = datasets.MNIST(root = \"../\", train = False, transform = transform, download = True)\n",
    "\n",
    "mnist_combined_loader = DataLoader(dataset = mnist_train + mnist_test, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQVAE().to(device)\n",
    "lpips_model = LPIPS().eval().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "recon_criterion = nn.MSELoss()\n",
    "disc_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr = 1e-5, betas = (0.5, 0.999))\n",
    "optimizer_g = torch.optim.Adam(model.parameters(), lr = 1e-5, betas = (0.5, 0.999))\n",
    "\n",
    "disc_step_start = 15000\n",
    "step_count = 0\n",
    "\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    for i, (im, _) in enumerate(celebALoader):\n",
    "        optimizer_d.zero_grad()\n",
    "        optimizer_g.zero_grad()\n",
    "        \n",
    "        step_count += 1\n",
    "        im = im.float().to(device)\n",
    "        \n",
    "        # Generator\n",
    "        model_output = model(im)\n",
    "        output, z, quantize_losses = model_output\n",
    "        \n",
    "        recon_loss = recon_criterion(output, im)\n",
    "        \n",
    "        g_loss = recon_loss + (1 * quantize_losses[\"codebook_loss\"]) + (0.2 * quantize_losses[\"commitment_loss\"])\n",
    "        \n",
    "        if step_count > disc_step_start:\n",
    "            disc_fake_pred = discriminator(output)\n",
    "            disc_fake_loss = disc_criterion(disc_fake_pred, torch.ones(disc_fake_pred.shape, device = disc_fake_pred.device))\n",
    "            \n",
    "            g_loss += 0.5 * disc_fake_loss\n",
    "        \n",
    "        lpips_loss = torch.mean(lpips_model(output, im))\n",
    "        g_loss += lpips_loss\n",
    "        g_loss.backward(retain_graph = True)\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        # Discriminator\n",
    "        if step_count > disc_step_start:\n",
    "            fake = output\n",
    "            disc_fake_pred = discriminator(fake.detach())\n",
    "            disc_real_pred = discriminator(im)\n",
    "            disc_fake_loss = disc_criterion(disc_fake_pred, torch.zeros(disc_fake_pred.shape, device = disc_fake_pred.device))\n",
    "            disc_real_loss = disc_criterion(disc_real_pred, torch.ones(disc_real_pred.shape, device = disc_real_pred.device))\n",
    "            \n",
    "            disc_loss = 0.5 * (disc_fake_loss + disc_real_loss) / 2\n",
    "            disc_loss.backward()\n",
    "            optimizer_d.step()\n",
    "        \n",
    "    print(f\"Epoch: [{epoch_idx + 1}/{num_epochs}]\")\n",
    "    torch.save(model.state_dict(), \"vqvae_autoencoder.pth\")\n",
    "    torch.save(discriminator.state_dict(), \"vqvae_discriminator.pth\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        grid_real = vutils.make_grid(im[:16], nrow = 4, normalize = True)\n",
    "        grid_reconstructed = vutils.make_grid(output[:16], nrow = 4, normalize = True)\n",
    "        grid_quantized = vutils.make_grid(z[:16], nrow = 4, normalize = True)\n",
    "        \n",
    "        plt.figure(figsize = (15, 15))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Real Images\")\n",
    "        plt.imshow(np.transpose(grid_real.cpu().detach().numpy(), (1, 2, 0)))\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Reconstructed Images\")\n",
    "        plt.imshow(np.transpose(grid_reconstructed.cpu().detach().numpy(), (1, 2, 0)))\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Quantized Images\")\n",
    "        plt.imshow(np.transpose(grid_quantized.cpu().detach().numpy(), (1, 2, 0)))\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Diffusion Model Architecture\n",
    "### UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, im_channels = 3):\n",
    "        super().__init__()\n",
    "        self.down_channels = [256, 384, 512, 768]\n",
    "        self.mid_channels = [768, 512]\n",
    "        self.t_emb_dim = 512\n",
    "        self.down_sample = [True, True, True]\n",
    "        self.num_down_layers = 2\n",
    "        self.num_mid_layers = 2\n",
    "        self.num_up_layers = 2\n",
    "        self.attns = [True, True, True]\n",
    "        self.norm_channels = 32\n",
    "        self.num_heads = 16\n",
    "        self.conv_out_channels = 128\n",
    "        \n",
    "        # Initial projection from sinusoidal time embedding\n",
    "        \n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
    "        )\n",
    "        \n",
    "        self.up_sample = list(reversed(self.down_sample))\n",
    "        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size = 3, padding = 1)\n",
    "        \n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels) - 1):\n",
    "            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i + 1],\n",
    "                                       t_emb_dim = self.t_emb_dim, down_sample = self.down_sample[i],\n",
    "                                       num_heads = self.num_heads, num_layers = self.num_down_layers,\n",
    "                                       attn = self.attns[i], norm_channels = self.norm_channels))\n",
    "        \n",
    "        self.mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels) - 1):\n",
    "            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1],\n",
    "                                    t_emb_dim = self.t_emb_dim, num_heads = self.num_heads,\n",
    "                                    num_layers = self.num_mid_layers, norm_channels = self.norm_channels))\n",
    "        \n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in reversed(range(len(self.down_channels) - 1)):\n",
    "            self.ups.append(UpBlockUnet(self.down_channels[i] * 2, self.down_channels[i - 1] if i != 0 else self.conv_out_channels,\n",
    "                                    self.t_emb_dim, up_sample = self.down_sample[i],\n",
    "                                        num_heads = self.num_heads,\n",
    "                                        num_layers = self.num_up_layers,\n",
    "                                        norm_channels = self.norm_channels))\n",
    "        \n",
    "        self.norm_out = nn.GroupNorm(self.norm_channels, self.conv_out_channels)\n",
    "        self.conv_out = nn.Conv2d(self.conv_out_channels, im_channels, kernel_size = 3, padding = 1)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # Shapes assuming downblocks are [C1, C2, C3, C4]\n",
    "        # Shapes assuming midblocks are [C4, C4, C3]\n",
    "        # Shapes assuming downsamples are [True, True, False]\n",
    "        # B x C x H x W\n",
    "        out = self.conv_in(x)\n",
    "        # B x C1 x H x W\n",
    "        \n",
    "        # t_emb -> B x t_emb_dim\n",
    "        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "        \n",
    "        down_outs = []\n",
    "        \n",
    "        for idx, down in enumerate(self.downs):\n",
    "            down_outs.append(out)\n",
    "            out = down(out, t_emb)\n",
    "        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n",
    "        # out B x C4 x H/4 x W/4\n",
    "        \n",
    "        for mid in self.mids:\n",
    "            out = mid(out, t_emb)\n",
    "        # out B x C3 x H/4 x W/4\n",
    "        \n",
    "        for up in self.ups:\n",
    "            down_out = down_outs.pop()\n",
    "            out = up(out, down_out, t_emb)\n",
    "            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n",
    "        out = self.norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.conv_out(out)\n",
    "        # out B x C x H x W\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Noise Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNoiseScheduler:\n",
    "    r\"\"\"\n",
    "    Class for the linear noise scheduler that is used in DDPM.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_timesteps, beta_start, beta_end):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        # Mimicking how compvis repo creates schedule\n",
    "        self.betas = (\n",
    "                torch.linspace(beta_start ** 0.5, beta_end ** 0.5, num_timesteps) ** 2\n",
    "        )\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
    "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n",
    "    \n",
    "    def add_noise(self, original, noise, t):\n",
    "        r\"\"\"\n",
    "        Forward method for diffusion\n",
    "        :param original: Image on which noise is to be applied\n",
    "        :param noise: Random Noise Tensor (from normal dist)\n",
    "        :param t: timestep of the forward process of shape -> (B,)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        original_shape = original.shape\n",
    "        batch_size = original_shape[0]\n",
    "        \n",
    "        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
    "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
    "        \n",
    "        # Reshape till (B,) becomes (B,1,1,1) if image is (B,C,H,W)\n",
    "        for _ in range(len(original_shape) - 1):\n",
    "            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n",
    "        for _ in range(len(original_shape) - 1):\n",
    "            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n",
    "        \n",
    "        # Apply and Return Forward process equation\n",
    "        return (sqrt_alpha_cum_prod.to(original.device) * original\n",
    "                + sqrt_one_minus_alpha_cum_prod.to(original.device) * noise)\n",
    "    \n",
    "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
    "        r\"\"\"\n",
    "            Use the noise prediction by model to get\n",
    "            xt-1 using xt and the nosie predicted\n",
    "        :param xt: current timestep sample\n",
    "        :param noise_pred: model noise prediction\n",
    "        :param t: current timestep we are at\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x0 = ((xt - (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t] * noise_pred)) /\n",
    "              torch.sqrt(self.alpha_cum_prod.to(xt.device)[t]))\n",
    "        x0 = torch.clamp(x0, -1., 1.)\n",
    "        \n",
    "        mean = xt - ((self.betas.to(xt.device)[t]) * noise_pred) / (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t])\n",
    "        mean = mean / torch.sqrt(self.alphas.to(xt.device)[t])\n",
    "        \n",
    "        if t == 0:\n",
    "            return mean, x0\n",
    "        else:\n",
    "            variance = (1 - self.alpha_cum_prod.to(xt.device)[t - 1]) / (1.0 - self.alpha_cum_prod.to(xt.device)[t])\n",
    "            variance = variance * self.betas.to(xt.device)[t]\n",
    "            sigma = variance ** 0.5\n",
    "            z = torch.randn(xt.shape).to(xt.device)\n",
    "            \n",
    "            # OR\n",
    "            # variance = self.betas[t]\n",
    "            # sigma = variance ** 0.5\n",
    "            # z = torch.randn(xt.shape).to(xt.device)\n",
    "            return mean + sigma * z, x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Diffusion Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Parameters\n",
    "batch_size = 64\n",
    "num_epochs = 50\n",
    "lr = 5e-5\n",
    "\n",
    "# Diffusion Parameters\n",
    "beta_start = 1e-4\n",
    "beta_end = 2e-2\n",
    "T = 1000\n",
    "\n",
    "# Model Parameters\n",
    "nc = 3\n",
    "image_size = 256\n",
    "z_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(image_size), transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize([0.5 for _ in range(nc)], [0.5 for _ in range(nc)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celebADataset = datasets.ImageFolder(root = \"../CelebA\", transform = transform)\n",
    "celebALoader = DataLoader(dataset = celebADataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "mnist_train = datasets.MNIST(root = \"../\", train = True, transform = transform, download = True)\n",
    "mnist_test = datasets.MNIST(root = \"../\", train = False, transform = transform, download = True)\n",
    "\n",
    "mnist_combined_loader = DataLoader(dataset = mnist_train + mnist_test, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = LinearNoiseScheduler(num_timesteps = T, beta_start = beta_start, beta_end = beta_end)\n",
    "\n",
    "model = Unet(im_channels = z_channels).to(device)\n",
    "model.train()\n",
    "\n",
    "vae = VQVAE().to(device)\n",
    "vae.eval()\n",
    "vae.load_state_dict(torch.load(\"vqvae_autoencoder.pth\", map_location = device))\n",
    "\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    losses = []\n",
    "    for i, (im, _) in enumerate(celebALoader):\n",
    "        optimizer.zero_grad()\n",
    "        im = im.float().to(device)\n",
    "        \n",
    "        # THE MAIN PART -> LATENT SPACE TRAINING\n",
    "        with torch.no_grad():\n",
    "            im, _ = vae.encode(im)\n",
    "        \n",
    "        noise = torch.randn_like(im).to(device)\n",
    "        t = torch.randint(0, T, (im.shape[0],)).to(device)\n",
    "        \n",
    "        noisy_im = scheduler.add_noise(im, noise, t)\n",
    "        noise_pred = model(noisy_im, t)\n",
    "        \n",
    "        loss = criterion(noise_pred, noise)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch_idx + 1}/{num_epochs}], Loss: {np.mean(losses)}\")\n",
    "    torch.save(model.state_dict(), \"denoiseLatentModelCeleb.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_grid_rows = 4\n",
    "num_samples = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = LinearNoiseScheduler(T, beta_start, beta_end)\n",
    "\n",
    "model = Unet(im_channels = z_channels).to(device)\n",
    "model.load_state_dict(torch.load(\"./denoiseLatentModelCeleb.pth\", map_location = device))\n",
    "model.eval()\n",
    "\n",
    "vae = VQVAE().to(device)\n",
    "vae.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    im_size = image_size // (2 ** (sum(model.down_sample)))\n",
    "    xt = torch.randn((num_samples, z_channels, im_size, im_size)).to(device)\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        noise_pred = model(xt, torch.as_tensor(t).unsqueeze(0).to(device))\n",
    "        xt, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(t).to(device))\n",
    "        \n",
    "        ims_raw = torch.clamp(xt, -1., 1.).detach().cpu()\n",
    "        ims_raw = (ims_raw + 1) / 2\n",
    "        \n",
    "        ims = vae.decode(xt)\n",
    "        ims = torch.clamp(ims, -1., 1.).detach().cpu()\n",
    "        ims = (ims + 1) / 2\n",
    "        \n",
    "        grid_latent = vutils.make_grid(ims_raw, nrow = num_grid_rows, normalize = True)\n",
    "        grid_reconstructed = vutils.make_grid(ims, nrow = num_grid_rows, normalize = True)\n",
    "        \n",
    "        if (t % 50 == 0):\n",
    "            plt.figure(figsize = (15, 15))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(np.transpose(grid_latent.cpu().detach().numpy(), (1, 2, 0)))\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(np.transpose(grid_reconstructed.cpu().detach().numpy(), (1, 2, 0)))\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "        img_latent = transforms.ToPILImage()(grid_latent)\n",
    "        img_decode = transforms.ToPILImage()(grid_reconstructed)\n",
    "        if not os.path.exists(\"./DDPM/CelebLatent\"):\n",
    "            os.makedirs(\"./DDPM/CelebLatent\")\n",
    "            \n",
    "        if not os.path.exists(\"../DDPM/CelebDecode\"):\n",
    "            os.makedirs(\"../DDPM/CelebDecode\")\n",
    "        \n",
    "\n",
    "        img_latent.save(f\"../DDPM/CelebLatent/x0_{t}.png\")\n",
    "        img_latent.close()\n",
    "        \n",
    "        img_decode.save(f\"../DDPM/CelebDecode/x0_{t}.png\")\n",
    "        img_decode.close()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
